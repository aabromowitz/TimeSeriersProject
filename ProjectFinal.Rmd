---
title: "Time Series Project"
author: "Aaron Abromowitz and Alex Thibeaux"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r, message=FALSE, "Load Libraries"}
library(tidyverse)
library(tswge)
library(vars)
library(lubridate)
library(vars)
library(nnfor)
library(caret)
```

# Load Data
```{r Load and wrangle data}

# Variable of Interest - Quarterly from 1/1/63
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSPUS.csv"
mhp <- read.csv(file_path, header = TRUE)

# Home ownership rate - Quarterly from 1/1/65
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/RHORUSQ156N.csv"
hor <- read.csv(file_path, header = TRUE)

# Housing units completed - Monthly from 1/1/1968
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/COMPUTSA.csv"
huc <- read.csv(file_path, header = TRUE)

# Supply of new houses - Monthly from 1/1/1963
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSACSR.csv"
snh <- read.csv(file_path, header = TRUE)

# House price index - Quarterly from 1/1/1975
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/USSTHPI.csv"
hpi <- read.csv(file_path, header = TRUE)

# Converting Monthly Data to Quarterly Data

# Preserve Monthly format
snh_monthly <- snh
huc_monthly <- huc

# Supply of New Houses Variable 
snh$DATE = as.Date(snh$DATE)
snh$month <- month(snh$DATE)
head(snh)
snh_quarterly <- snh %>%
  filter(snh$month == 1 | snh$month == 4 | snh$month == 7 | snh$month == 10)
summary(snh_quarterly)

# Housing Units Completed Variable
huc$DATE = as.Date(huc$DATE)
huc$month <- month(huc$DATE)
head(huc)
huc_quarterly <- huc %>%
  filter(huc$month == 1 | huc$month == 4 | huc$month == 7 | huc$month == 10)
summary(huc_quarterly)

# Using same time frames, which would be starting at 1975 Q1 and ending at 2024 Q2 (due to hpi data)

# hor observation 41 is 1975 Q1
hor_1975 = hor[41:238,]
hor_1975$DATE <- as.Date(hor_1975$DATE)
summary(hor_1975)

# huc_quarterlly observation 29
huc_1975 = huc_quarterly[29:226,]
summary(huc_1975)

# mhp observation 49 is 1975 Q1
mhp_1975 = mhp[49:246,]
mhp_1975$DATE <- as.Date(mhp_1975$DATE)
summary(mhp_1975)

# snh_quarterly observation 49 is 1975 Q1
snh_1975 = snh_quarterly[49:246,]
summary(snh_1975)

# Housing Price Index variable already from 1975 Q1 - 2024 Q2
hpi$DATE <- as.Date(hpi$DATE)
summary(hpi)

# Create Dataframe
# Combined (Train/Test) Set NOT LOGGED
fed_housing_data_NL = data.frame(Year_Quarter = mhp_1975$DATE, Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = mhp_1975$MSPUS)

# Combined (Train/Test) Set LOGGED
fed_housing_data = data.frame(Year_Quarter = as.Date(mhp_1975$DATE), Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = log(mhp_1975$MSPUS))

# Train & Test sets
train = fed_housing_data[1:168,]
test = fed_housing_data[169:198,]

summary(fed_housing_data)

# plotting variables
xmin_plot = 150
ymin_plot = 12.3
ymax_plot = 13.1
ymax_future = 13.4
```

# EDA

## Median Housing Sale Price

Our variable of interest is the Median Housing Sale Price. This was quarterly data starting in 1965.

```{r Plot Median Housing Sales Price}
mhp=mhp$MSPUS
plot(ts(mhp/1000, frequency=4,start=c(1965,1)),xlab='Year',ylab='Housing price (in thousands of dollars)')
title(main='Median US Housing Price from 1965')
acf(mhp,ylab='',main='ACF')
x = parzen.wge(mhp)
```

The realization looks like it is increasing linearly. This data could be model with a signal plus noise, with a linear signal.

The realization appears to be non stationary. There is evidence that the data is increasing over time. In addition, the variation appears much higher in later years as in earlier years, showing that variance is increasing over time as well. When modeling, it may be useful to take the logarithm of the data before modeling.

The ACF shows very slowly dampening autocorrelations. The parzen window also shows a very low frequency. This could points to a (1-B) term which could be removed with a high-pass difference filter.

To fix the problem with the increasing variance.  We decided to take the log of the Mean Housing Price.

```{r Plot log Median Housing Sales Price}
plot(ts(log(mhp), frequency=4,start=c(1965,1)),xlab='Year',ylab='Log Housing price')
title(main='Log Median US Housing Price from 1965')
```

## Home Ownership Rate

We used 4 exogenous variables. The first was home ownership rate. Like the variable of interest, this was quarterly data starting from 1965.

```{r Plot Home Ownership Rate}
hor <- hor$RHORUSQ156N
plot(ts(hor, frequency=4,start=c(1965,1)),xlab='Year',ylab='Percentage')
title(main='Home Ownership Rate from 1965')
acf(hor,ylab='',main='ACF')
x = parzen.wge(hor)
```

Unlike the median house price, the home ownership rate appears to be stationary. There isn’t a linear trend to the data, and the variance seems to be constant over time.

The ACF does show that there are dampening autocorrelations, but not quite at the rate as for median house price. This could point to an ARMA model being appropriate for the home ownership rate.

Similar to median house price, the Parzen Window shows that 0 is a prominent frequency.

## Housing Units Completed

The next exogenous variable is Housing Units Completed. This was a monthly variable, starting from 1968.

```{r Plot Housing Units Complete}
huc <- huc$COMPUTSA
plot(ts(huc, frequency=12,start=c(1968,1)),xlab='Year',ylab='')
title(main='Housing Units Completed from 1968')
acf(huc,ylab='',main='ACF')
x = parzen.wge(huc)
```

This has very similar characteristics to home ownership rate. It’s autocorrelations dampen a little more slowly, but the ACF and Parzen Window look very similar. This variable likewise shows evidence for being stationary.

## Supply of New Houses

```{r Plot Supply of New Houses}
snh <- snh$MSACSR
plot(ts(snh, frequency=12,start=c(1963,1)),xlab='Year',ylab='')
title(main='Supply of New Houses from 1963')
acf(snh,ylab='',main='ACF')
x = parzen.wge(snh)
```

Similar to home ownership rate as well, but with even more quickly dampening autocorrelations. This variable likewise shows evidence for being stationary.

## House Price Index

The next exogenous variable is House Price Index. This was a quarterly variable, starting from 1975.

```{r Plot House Price Index}
hpi <- hpi$USSTHPI
plot(ts(hpi, frequency=4,start=c(1975,1)),xlab='Year',ylab='')
title(main='House Price Index from 1975')
acf(hpi,ylab='',main='ACF')
x = parzen.wge(hpi)
```

The realization has a similar upward trend to the Housing Sale Price. It seems very smooth though, very little variance. Because of the upward trend though, it shows evidence against stationarity.

The ACF and Parzen Window are also similar to those of Housing Sale Price.

Not all the data matches up by date range or by frequency of observation (monthly vs quarterly).  To make everything consistent, we will only use quarterly data going from Q1 1975 to Q2 2024.


# AR(I)MA Model Investigation

## First ARIMA Model

The first model we looked at was an ARIMA model.  From the EDA, we saw that because of the increasing variance, we should look at the log of the Housing Sales Price.  And since the Parzen Window had a prominent peak at 0 and very slowly dampening autocorrelations, we should take the first difference.

```{r Difference Median Housing Price}
log.mhp = fed_housing_data$Median_Sales_Price
mhp = exp(log.mhp)
d.log.mhp = artrans.wge(log.mhp,1)
```

The difference looks more like white noise without an obvious pattern to the ACF.  Next we will try modeling the differenced data as an ARMA model.

```{r ARIMA(p,1,q) p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 2.

We will evaluate our models with a short term horizon of 1 year (4 quarters) and a long term horizon of 5 years (20 quarters).  We calcluated a Rolling Window RMSE for both of those horizons (using the logged data) and calculated an ASE for both of those horizons as well (taking the exponential to get back to the original data).

```{r ARIMA(p,1,q) metrics}
h.short = 4
h.long = 20
l = length(mhp)
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 84.37902
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 7091.032
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=1,phi=est$phi,theta=est$theta) # 0.036501
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=1,phi=est$phi,theta=est$theta) # 0.13269
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,1,2) model we get a short term Rolling Window RMSE of 0.037, a long term Rolling Window RMSE of 0.133, a short term ASE of 84.4 million and a long term ASE of 7.09 billion.

## ARMA Model

As a comparison, we wanted to look a simpler ARMA model.  

```{r ARMA p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='bic')
```

Both the AIC and BIC selection choose p = 2 and q = 2.

```{r ARMA metrics}
est = est.arma.wge(log.mhp,p=2,q=2)
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 112.4314
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 9167.59
```
```{r ARMA Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,phi=est$phi,theta=est$theta) # 0.044488
```
```{r ARMA Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARMA Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,phi=est$phi,theta=est$theta) # 0.12779
```
```{r ARMA Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARMA(2,2) model we get a short term Rolling Window RMSE of 0.044, a long term Rolling Window RMSE of 0.128, a short term ASE of 112.4 million and a long term ASE of 9.17 billion.  Even though this model has a better long term Rolling Window RMSE than the ARIMA model, it did worse in the other metrics.

## ARIMA(p,2,q) model

Looking at the overfit factor table, we can see two prominent 0 frequency roots.  

```{r factor table}
est = est.ar.wge(log.mhp,p=12)
factor.wge(phi=est$phi)
```

Even though the differenced data didn't have slowly dampening autocorrelations and looked liked white noise, the original data had a pretty clear upward trend.  We thought that this warranted investigation into an ARIMA(p,2,q) model.

```{r ARIMA(p,2,q) p and q estimation}
d2.log.mhp = artrans.wge(d.log.mhp,1)
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 1.

```{r ARIMA(1,2,1) metrics}
est = est.arma.wge(d2.log.mhp,p=1,q=1)
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 162.1383
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 6263.556
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=2,phi=est$phi,theta=est$theta) # 0.049161
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=2,phi=est$phi,theta=est$theta) # 0.1809277
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,2,1) model we get a short term Rolling Window RMSE of 0.049, a long term Rolling Window RMSE of 0.181, a short term ASE of 162.4 million and a long term ASE of 6.26 billion.  Even though this model had the best long term ASE so far, it did worse in the other metrics.

Comparing all three ARMA/ARIMA models, we will choose the ARIMA(1,1,2). Here are what the short and long term forecasts look like zoomed in.

```{r ARIMA(1,1,2) plots for PPT}
h.short = 4
h.long = 20
x = fed_housing_data$Median_Sales_Price
l = length(x)
d = artrans.wge(x,1)
dev.off()
est = est.arma.wge(d,p=1,q=2)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE,plot=FALSE)
plot(seq(xmin_plot,l,1),x[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="ARIMA(1,1,2) Short Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.short+1),l,1),f$f,col="red")
lines(seq((l-h.short+1),l,1),f$ll,col="blue",lty=3) # lty=3 for dotted line
lines(seq((l-h.short+1),l,1),f$ul,col="blue",lty=3)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE,plot=FALSE)
plot(seq(xmin_plot,l,1),x[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="ARIMA(1,1,2) Long Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.long+1),l,1),f$f,col="red")
lines(seq((l-h.long+1),l,1),f$ll,col="blue",lty=3)
lines(seq((l-h.long+1),l,1),f$ul,col="blue",lty=3)
```

```{r ARIMA(1,1,2) model characteristics}
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE,plot=FALSE)
aic = est$aic
aic # -7.188559
resid = f$resid
xbar=est$xbar
xbar # 0.01211603
vara = est$avar
vara # 0.0007251235
est$phi # 0.6901326
est$theta # 0.9720137 -0.4007360
```

The model (for the log data) can be written as:
(1-B)(1-0.69B)(x_t-0.012) = (1-0.972B+0.401B^2)a_t 
$\sigma$_t^2 = 0.001

Now we will look at residuals to make sure that they are white noise.

```{r ARIMA(1,1,2) residual plots}
plotts.wge(resid)
title(main="Residuals")
acf(resid,lag.max=100,main="ACF of Residuals") 
parzen.wge(resid)
```

They appear to be white noise, and the ACF has very few values above 0.2.  We will run a Ljung-Box test to be sure.

```{r ARIMA(1,1,2) Ljung test}
ljung.wge(resid,K=24,p=1,q=2) # p = 0.2272667, white
ljung.wge(resid,K=48,p=1,q=2) # p = 0.5584003, white
```

The test gives more evidence that the residuals are white noise.  We can also check if they are normally distributed with a histogram.

```{r ARIMA(1,1,2) residual histogram}
hist(resid)
```

We also want to make sure that the ACFs and Parzen Windows generated from ARIMA(1,2,2) models look similar to that of our original data.

```{r ARIMA(1,1,2) generated ACFs and Parzen Windows}
# Multiple ACFs 
set.seed(2)
sims = 10
ACF = acf(log.mhp, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6,xlab="Lag",ylab="ACF",main="True ACF vs Generated Data")
for( i in 1: sims)
{
  ACF2 = acf(gen.arima.wge(l, phi = est$phi, theta=est$theta,d=1, plot="FALSE"), plot = "FALSE")
  lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

# Multiple Parzen 
set.seed(3)
sims = 10
SpecDen = parzen.wge(log.mhp, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6,xlab="Frequency",ylab="dB",main="True Spectral Density vs Generated Data")
for( i in 1: sims)
{
  SpecDen2 = parzen.wge(gen.aruma.wge(l,phi=est$phi, theta=est$theta,d=1, plot ="FALSE"), plot = "FALSE")
  lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}
```

The ACFs don't match up exactly, but have the same slowly dampening behavior.  The Parzen Windows are very close to the original.

We will look at the short and long term forecasts of future data as well.

```{r ARIMA(1,1,2) future forecasts}
h.short = 4
h.long = 20
x = fed_housing_data$Median_Sales_Price
l = length(x)
d = artrans.wge(x,1)
dev.off()
est = est.arma.wge(d,p=1,q=2)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=FALSE,plot=FALSE)
plot(seq(xmin_plot,l+h.short,1),x[xmin_plot:(l+h.short)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="ARIMA(1,1,2) Short Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.short),1),f$f,col="red")
lines(seq((l+1),(l+h.short),1),f$ll,col="blue",lty=3) # lty=3 for dotted line
lines(seq((l+1),(l+h.short),1),f$ul,col="blue",lty=3)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=FALSE,plot=FALSE)
plot(seq(xmin_plot,l+h.long,1),x[xmin_plot:(l+h.long)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="ARIMA(1,1,2) Long Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.long),1),f$f,col="red")
lines(seq((l+1),(l+h.long),1),f$ll,col="blue",lty=3) # lty=3 for dotted line
lines(seq((l+1),(l+h.long),1),f$ul,col="blue",lty=3)
```

Unsurprisingly, the future forecasts didn't increase with time.  This is similar to the forecasts we used for predictions.

# Exogenous Variable Forecasts

In order to use the Exogenous variables in either the MLR or Multi-Variate MLP, it is more realistic to make predictions for them first.  

For error metrics, we will focus on the ASE for short and long term predictions.  This is because we are specifically using these short and long term predictions in future models.

## Home Ownership Rate

The Home Ownership Rate looked like either an ARMA or an ARIMA(p,1,q) might be appropriate.

```{r re-plot Home Ownership Rate}
x = fed_housing_data$Ownership_Rate 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```

We will start with the ARIMA model.

```{r AIC for Home Ownership Rate ARIMA}
aic5.wge(d,p=0:10,q=0:4,type='aic') # 8/1 best
```

The model with the best AIC is p = 8, q = 1.

```{r Home Ownership Rate ARIMA(8,1,1) metrics}
est = est.arma.wge(d,p=8,q=1)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.03083697
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 2.077416
```

This model has a short term ASE of 0.031 and a long term ASE of 2.08.

Next will compare this to an ARMA model.

```{r AIC for Home Ownership Rate ARMA}
aic5.wge(x,p=0:12,q=0:2,type='aic') # 9/1 best
```

The model it chose had p = 9 and q = 1.

```{r Home Ownership Rate ARMA(9,1) metrics}
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.02468854
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 1.108741
```

This model had better ASE values with 0.025 for short term and 1.11 for long term.  This will be the predictions we use for our MLR and MLP models.

```{r Home Ownership Rate creating forecast predictions}
fed_housing_data_short = fed_housing_data
fed_housing_data_long = fed_housing_data
x = fed_housing_data$Ownership_Rate 
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
hor.pred.short = f$f
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
hor.pred.long = f$f
fed_housing_data_short$Ownership_Rate[(l-h.short+1):l] = hor.pred.short
fed_housing_data_long$Ownership_Rate[(l-h.long+1):l] = hor.pred.long
```

## Housing Units Completed

The Housing Units Completed variable looked like either an ARMA or an ARIMA(p,1,q) might be appropriate for forecasting.

```{r re-plot Housing Units Completed}
x = fed_housing_data$Housing_Units_Completed 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```

However, this looks like the autocorrelations dampen even less slowly than the Home Ownership Rate, meaning that an ARIMA(p,1,q) model would be even less appropriate.  Since we chose the ARMA model for Home Ownership Rate over the ARIMA model.  We will just focus on the ARMA model for Housing Units Complete.

```{r AIC for Housing Units Completed}
aic5.wge(x,p=0:10,q=0:1,type='aic') # 10/0 best
```

The highest AIC value was for p = 10 and q = 0.

```{r Housing Units Completed AR(10) metrics}
est = est.ar.wge(x,p=10)
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)/1e3
ase # 12.98288
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)/1e3
ase # 8.546039
```

This gives a short term ASE of 13.0k and a long term ASE of 8.5k.  Playing around with some of the other output from the aic5 output showed that an ARMA(9,1) model does slightly better for long term predictions and slightly worse for short term predictions.

```{r Housing Units Completed ARMA(9,1) metrics}
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)/1e3
ase # 13.44075
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)/1e3
ase # 8.516217
```

We will proceed with the ARMA(9,1) forecast for our future models that use exogenous variables.

```{r Housing Units Completed creating forecast predictions}
x = fed_housing_data$Housing_Units_Completed  
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
huc.pred.short = f$f
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
huc.pred.long = f$f
fed_housing_data_short$Housing_Units_Completed[(l-h.short+1):l] = huc.pred.short
fed_housing_data_long$Housing_Units_Completed[(l-h.long+1):l] = huc.pred.long
```

## Supply of New Houses

Similar to Housing Units Completed, an ARMA model seems like it would be most appropriate for Supply of New Houses.

```{r re-plot Supply of New Houses}
x = fed_housing_data$Supply_New_Houses 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```

```{r AIC for Supply of New Houses}
aic5.wge(x,p=0:4,q=0:1,type='aic') # 1/0 best
```

The highest AIC value was for p = 1 and q = 0.

```{r Supply of New Houses AR(1) metrics}
est = est.ar.wge(x,p=1)
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.6951032
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 3.865913
```

This gives a short term ASE of 13.0k and a long term ASE of 8.5k.  Playing around with some of the other output from the aic5 output showed that an ARMA(9,1) model does slightly better for long term predictions and slightly worse for short term predictions.

```{r Supply of New Houses ARMA(1,1) metrics}
est = est.arma.wge(x,p=1,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.4791473
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 3.901133
```

The short term ASE for the ARMA(1,1) is significantly better, but the long term ASE is a little worse.  We will proceed with the ARMA(1,1) model.

```{r Supply of New Houses creating forecast predictions}
x = fed_housing_data$Supply_New_Houses   
est = est.arma.wge(x,p=1,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
snh.pred.short = f$f
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
snh.pred.long = f$f
fed_housing_data_short$Supply_New_Houses[(l-h.short+1):l] = snh.pred.short
fed_housing_data_long$Supply_New_Houses[(l-h.long+1):l] = snh.pred.long
```

## House price index

```{r re-plot House price index}
x = fed_housing_data$Housing_Price_Index
plotts.sample.wge(x)
d = artrans.wge(x,1)
d2 = artrans.wge(d,1)
dev.off()
parzen.wge(d2)
d3 = artrans.wge(d2,c(0,-1))
dev.off()
acf(d3,lag.max=100)
```

This plot is more similar to the variable of interest, Medium Sale Price.   This means that a linear signal plus noise model could be useful.

However, unlike the other variables, it looked like a single difference wasn't white noise.  Even differencing out the (1-B)^2 still had a 0.25 frequency.  Differencing out that frequency as well looked more like white noise.  So we will look at a signal plus noise model and a (1-B)^2*(1+B^2) model.

```{r House price index signal plus noise AIC}
f = fore.sigplusnoise.wge(x,max.p=0,n.ahead=h.short,lastn=TRUE)
aic5.ar.wge(f$resid,p=0:20) # p = 9
```

The model for the noise with the highest AIC value is p = 9.

```{r House price index signal plus noise ASE}
f = fore.sigplusnoise.wge(x,max.p=9,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 161.2832
f = fore.sigplusnoise.wge(x,max.p=9,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 10014.69
```

The short term ASE is 161 and the long term ASE is 10,014.

```{r House price index signal plus noise metrics}
aic5.wge(d3,p=0:8,q=0:6,type='aic') # 4/5 best
```

It looks like the model with p = 4 and q = 5 had the best AIC.

```{r House price index ARMA(4,5) metrics}
est = est.arma.wge(d3,p=4,q=5)
m = mult.wge(fac1=est$phi,fac2=c(0,-1))
factor.wge(m$model.coef) # looks good
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 16.01932
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 2109.469
```

This model did significantly better with a short term ASE of 16 and a long term ASE of 2,109.  Testing out some of the other values from aic5, found that p = 6 and q = 5 did even better.

```{r House price index ARMA(6,5) metrics}
est = est.arma.wge(d3,p=6,q=5)
m = mult.wge(fac1=est$phi,fac2=c(0,-1))
factor.wge(m$model.coef) # looks good
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 14.86637
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 558.2356
```

This model had a short term ASE of 15 and a long term ASE of 558.  This is the model we will use to get the forecasts.

```{r House price index creating forecast predictions}
x = fed_housing_data$Housing_Price_Index 
d = artrans.wge(x,1)
d2 = artrans.wge(d,1)
d3 = artrans.wge(d2,c(0,-1))
est = est.arma.wge(d3,p=6,q=5)
m = mult.wge(fac1=est$phi,fac2=c(0,-1))
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.short,lastn=TRUE)
hpi.pred.short = f$f
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.long,lastn=TRUE)
hpi.pred.long = f$f
fed_housing_data_short$Housing_Price_Index[(l-h.short+1):l] = hpi.pred.short
fed_housing_data_long$Housing_Price_Index[(l-h.long+1):l] = hpi.pred.long
```

# Signal Plus Noise Model

Our domain knowledge tells us that the positive trend of Median House Price is not random and does have an underlying regression line, but we can run a few tests for linear trend to be sure. We will test for linear trend using simple linear regression with a standard t-test, the Cochran Orcutt Test for Linear Trend, and the WBG Test (Woodward, Bottone, and Gray, 1997). All 3 tests have a p-value below 0.05, rejecting the null hypothesis. The residuals are not white noise yet, so we will fit an ARMA model along with the linear signal.

## Test for linear Trend

```{r Test for linear trend}
# Testing for Linear Trend
t = 1:168
reg = slr.wge(train$Median_Sales_Price)
summary(reg)

# Cochran Orcutt Test for Linear Trend
co_test_msp = co.wge(train$Median_Sales_Price)
co_test_msp$pvalue

# WBG Test
wbg.boot.wge(train$Median_Sales_Price)

# Linear model according to Simple Linear Regression (ignoring correlated errors)
plotts.wge(train$Median_Sales_Price)
fit = reg$b0hat + t*reg$b1hat
points(fit, type = "l")

# Examine Residuals
resid = train$Median_Sales_Price - fit
plot(resid, type = "p")
abline(h=0)

# Quicker way to plot residuals: plot(reg$res, type = "l")
```
```{r signal plus noise all residuals}
x = fed_housing_data
reg = slr.wge(x$Median_Sales_Price)
plot(reg$res, type = "l")
abline(h=0)

t=1:198
plotts.wge(x$Median_Sales_Price,xlab="Time",ylab="log Median House Price",main="Linear fit to log Median House Price")
fit = reg$b0hat + t*reg$b1hat
points(fit, type = "l")

aic5.ar.wge(reg$res,p=0:10)
```

## Choosing estimates for the ARMA portion of the Signal Plus Noise Model

After comparing the MLE and Burg estimates on AIC, AICC, and BIC, we determined the MLE estimates were better for all three metrics. All estimates chose an AR(6), with the exception of the MLE estimate using the BIC, which chose an AR(2). We will move forward with the AR(6) model, compare the forecasts visually, and test whether the Average Square Error (ASE) is better for the MLE or the Burg estimates. The MLE estimates have better ASEs, as expected. While there isn't much difference visually between the two, the MLE lower limit (blue line) does contain the actual values, whereas the Burg lower limit (green line) are slightly more narrow and doesn't capture one of the actual values. Given the other evidence for using the MLE estimates, we will move forward with MLE estimates for an AR(6).

```{r Signal Plus Noise models}
# Fit signal plus noise models

# MLE Estimates with AIC
spn.mle.aic = aic.ar.wge(reg$res, type = 'aic', p = 0:7) # ar(6), aic -7.269686

# MLE Estimates with AICC
spn.mle.aicc = aic.ar.wge(reg$res, type = 'aicc', p = 0:7) # ar(6), aicc -6.25239

# MLE Estimates with BIC
spn.mle.bic = aic.ar.wge(reg$res, type = 'bic', p = 0:7) # ar(2), bic -7.147245

# Burg Estimates with AIC
spn.b.aic = aic.burg.wge(reg$res, p = 1:7, type = 'aic') # ar(6), aic -7.278742

# Burg Estimates with AICC
spn.b.aicc = aic.burg.wge(reg$res, p = 1:7, type = 'aicc') # ar(6), aicc -6.261446

# Burg Estimates with BIC
spn.b.bic = aic.burg.wge(reg$res, p = 1:7, type = 'bic') # ar(6), bic -7.157681

# MLE estimates are better for all types: aic, aicc, bic - see comparison table

# Fit with MLE estimates, train/test set
fit.mle.sig = fore.sigplusnoise.wge(train$Median_Sales_Price, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 30)

# Examine residuals
plot(fit.mle.sig$resid)

# ASE with MLE estimates
ASE = mean((test$Median_Sales_Price - fit.mle.sig$f)^2)
ASEexp = mean((exp(test$Median_Sales_Price) - exp(fit.mle.sig$f))^2) # 826.5M

# Fit with burg estimates, train/test set
fit.b.sig = fore.sigplusnoise.wge(train$Median_Sales_Price, linear = TRUE, method = 'burg', freq = 0, max.p = 6, n.ahead = 30)

# Examine residuals
plot(fit.b.sig$resid)

# ASE with Burg estimates
ASE.b = mean((test$Median_Sales_Price - fit.b.sig$f)^2)
ASE.b.exp = mean((exp(test$Median_Sales_Price) - exp(fit.b.sig$f))^2) # 1.11B

# Create table for comparison
labels = c("aic", "aicc", "bic", "ase for AR(6)")
mle.ests = c(spn.mle.aic$value, spn.mle.aicc$value, spn.mle.bic$value, ASE)
burg.ests = c(spn.b.aic$value, spn.b.aicc$value, spn.b.bic$value, ASE.b)
comparison = data.frame(labels,mle.ests,burg.ests)
comparison

# Different Plot
log.mhp = fed_housing_data$Median_Sales_Price
plot(log.mhp, type = 'l')
lines(seq(169,198,1), fit.mle.sig$f, col = "red")
lines(seq(169,198,1), fit.mle.sig$ll, col = "blue")
lines(seq(169,198,1), fit.mle.sig$ul, col = "blue")
lines(seq(169,198,1), fit.b.sig$f, col = "orange")
lines(seq(169,198,1), fit.b.sig$ll, col = "green")
lines(seq(169,198,1), fit.b.sig$ul, col = "green")

```

## Signal Plus Noise using 1 Yr Horizon

The 1 Year Horizon backcast (for the past year) is pretty close to the actual values, with an ASE (on the non-lagged values) around 50M. The residuals appear to be white noise when looking at both the plot and the ACF, indicating this model has explained most of the noise of the data.

```{r Signal Plus Noise with 1 yr Horizon}
# Making sure log.mhp is the correct data
log.mhp = fed_housing_data$Median_Sales_Price

# Fit with MLE estimates, using all data
fit.mle.sig_h4 = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 4, lastn = TRUE)

# Different Plot
plot(log.mhp, type = 'l')
lines(seq(195,198,1), fit.mle.sig_h4$f, col = "red")
lines(seq(195,198,1), fit.mle.sig_h4$ll, lty = 3, col = "blue")
lines(seq(195,198,1), fit.mle.sig_h4$ul, lty = 3, col = "blue")

# Zoomed In Plot
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim = c(12.3, 13.2), main = "Linear Signal with AR(6) Noise Short Term Forecast", 
     xlab = "Time", ylab = "log Median Housing Sales Price")
lines(seq(195,198,1), fit.mle.sig_h4$f, col = "red")
lines(seq(195,198,1), fit.mle.sig_h4$ll, lty = 3, col = "blue")
lines(seq(195,198,1), fit.mle.sig_h4$ul, lty = 3, col = "blue")

# ASE with MLE estimates
ASE.h4 = mean((log.mhp[195:198] - fit.mle.sig_h4$f)^2)
ASEexp.h4 = mean((exp(log.mhp[195:198]) - exp(fit.mle.sig_h4$f))^2)
ASEexp.h4 # 50.92M

# Examine residuals
plot(fit.mle.sig_h4$resid)
plotts.sample.wge(fit.mle.sig_h4$resid)
acf(fit.mle.sig_h4$resid)
ljung.wge(fit.mle.sig_h4$resid, p = 6, q = 0, K = 48)
```
## Signal Plus Noise using 5 Yr Horizon

The 5 Year Horizon backcast (for the past 5 years) is pretty close to the actual values, with an ASE (on the non-lagged values) around 1.1B. The residuals appear to be white noise when looking at both the plot and the ACF, indicating this model has explained most of the noise of the data.

```{r Signal plus noise with 5 yr horizon}
# Making sure log.mhp is the correct data
log.mhp = fed_housing_data$Median_Sales_Price

# Fit with MLE estimates, using all data
fit.mle.sig_h20 = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 20, lastn = TRUE)

# Different Plot
plot(log.mhp, type = 'l')
lines(seq(179,198,1), fit.mle.sig_h20$f, col = "red")
lines(seq(179,198,1), fit.mle.sig_h20$ll, lty = 3, col = "blue")
lines(seq(179,198,1), fit.mle.sig_h20$ul, lty = 3, col = "blue")

# Zoomed In
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim = c(12.3, 13.2), main = "Linear Signal with AR(6) Noise Long Term Forecast", 
     xlab = "Time", ylab = "log Median Housing Sales Price")
lines(seq(179,198,1), fit.mle.sig_h20$f, col = "red")
lines(seq(179,198,1), fit.mle.sig_h20$ll, lty = 3, col = "blue")
lines(seq(179,198,1), fit.mle.sig_h20$ul, lty = 3, col = "blue")

# ASE with MLE estimates
ASE.h20 = mean((log.mhp[179:198] - fit.mle.sig_h20$f)^2)
ASEexp.h20 = mean((exp(log.mhp[179:198]) - exp(fit.mle.sig_h20$f))^2)
ASEexp.h20 # 1.1B

# Examine residuals
plot(fit.mle.sig_h20$resid)
plotts.sample.wge(fit.mle.sig_h20$resid)
acf(fit.mle.sig_h20$resid)
ljung.wge(fit.mle.sig_h20$resid, p = 6, q = 0, K = 48)
```

## Rolling Window RMSE for SPN

To compare this model with other models, we will use code that Aaron has written to create a rolling window that measures RMSE for the model. We will use the MLE estimates for p values of 1-6 to assure ourselves that an AR(6) is the best AR model to accompany the signal. The average RMSE for all 1 year windows (4 quarters) is 0.0326, and the average RMSE for all 5 year windows (20 quarters) is 0.07595. 

```{r Rolling Window RMSE, error = F}
# Aaron's Rolling Window RMSE Code

# If you wanted to use all the fore.sigplusnoise.wge parameters, it would look something like this:
series = log.mhp
horizon = 12
linear = TRUE
method = "mle"
freq=0
max.p=5

# Rolling Window for 1 Year Horizon with MLE
horizon = 4
method = "mle"
source("functions_Aaron.R")
mle.p1h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=1) # 0.03564353
mle.p2h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=2) # 0.03547613
mle.p3h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=3) # 0.03495764
mle.p4h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=4) # 0.03326752
mle.p5h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=5) # 0.03261551
mle.p6h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=6) # 0.03259507
mle.p7h4 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=7) # 0.03259507

# Rolling Window for 5 Year Horizon  with MLE
horizon = 20
source("functions_Aaron.R")
mle.p1h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=1) # 0.08107048
mle.p2h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=2) # 0.08065467
mle.p3h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=3) # 0.07973718
mle.p4h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=4) # 0.07797091
mle.p5h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=5) # 0.07669397
mle.p6h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=6) # 0.0759512
mle.p7h20 = roll.win.rmse.linplusnoise.ada(series, horizon, max.p=7) # 0.0759512
```

## Evaluating Models

We will evaluate the model by comparing the ACFs and the Spectral Densities for the same model on generated signal plus noise data. The loop given in class for Unit 11 does not work with signal plus noise generated realizations because the output includes the AR portion of the model regardless of whether plot = TRUE or FALSE. Thus, we will generate 10 different realizations and then take the ACFs and Spectral Densities of all 10 objects. The 

```{r Ten generated realizations, include = FALSE }

# 10 generated realizations 
# Rename variable in working RMD that was not carried over to final RMD:
fit.mle.sig_notTT = fit.mle.sig

spn.mle.ar6.1 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.2 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.3 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.4 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.5 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.6 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.7 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.8 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.9 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)
spn.mle.ar6.10 = gen.sigplusnoise.wge(n=198, b0 = fit.mle.sig_notTT$b0hat, 
                                   b1 = fit.mle.sig_notTT$b1hat,
                                   phi=fit.mle.sig_notTT$phi.z, 
                                   vara = fit.mle.sig_notTT$wnv, plot = F)

ten.generated = list(spn.mle.ar6.1, spn.mle.ar6.2, spn.mle.ar6.3, spn.mle.ar6.4, spn.mle.ar6.5, spn.mle.ar6.6, spn.mle.ar6.7, spn.mle.ar6.8, spn.mle.ar6.9, spn.mle.ar6.10)

# ten.generated[[1]]
# parzen.wge(ten.generated[[1]])

```

```{r Comparing ACFs and Spectral Densities}
# Compare Spectral Densities
sims = 10
SpecDen = parzen.wge(log.mhp, plot = FALSE)
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 3,xlab="Frequency",ylab="dB",main="True Spectral Density vs Generated Data")

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(ten.generated[[i]], plot = FALSE)
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}

#Compare ACFs
sims = 10
ACF = acf(log.mhp, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 4,xlab="Lag",ylab="ACF",main="True ACF vs Generated Data")

for( i in 1: sims)
{
   ACF2 = acf(ten.generated[[i]], plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 1, col = "red")
}

# Residuals
plot(fit.mle.sig$resid, xlab="Time", ylab="", main="Residuals",type="b",col = "black", pch = 19)
acf(fit.mle.sig$resid,main="ACF of Residuals",lag.max=100)
```


## Model Choice

The MLE estimates for forecasting had better ASE and better rolling window RMSE for all three forecasts (1 yr, 3 yr, and 5 yr)

Our final Signal Plus Noise Model is Xt = 10.871 + .011t + zt, where (1 - .716B - .367B2 - 0.137B3 + 0.062B4 + .076B5 + .118B6)Zt with sigma2 = 0.0006929526

Wording: 

In 1 year, we are 95% confident that the median home sale price will be between $387,759 (e^12.86814) and $468,321 (e^13.05691). Our best estimate is $426,142 (e^12.96253).

In 5 years, we are 95% confident that the median home sale price will be between $419,048 (e^12.94574) and $669,482 (e^13.41426). Our best estimate is $529,665 (e^13.18).

```{r Model Choice}

# Fit with MLE estimates, using all data, forecasting ahead 1 year
fit.mle.sig_h4_ahead = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 4, lastn = FALSE)

# Fit with MLE estimates, using all data, forecasting ahead 5 years
fit.mle.sig_h20_ahead = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 20, lastn = FALSE)

# Checking if models all have same intercept, slope, and phis
fit.mle.sig_h4$b0hat == fit.mle.sig_h4_ahead$b0hat
fit.mle.sig_h4$b1hat == fit.mle.sig_h4_ahead$b1hat
fit.mle.sig_h4$phi.z == fit.mle.sig_h4_ahead$phi.z

# Model Coefficients
fit.mle.sig_h4$phi.z
fit.mle.sig_h4$b0hat
fit.mle.sig_h4$b1hat
fit.mle.sig_h4$wnv

# Confidence Intervals

# Find forecasted Values
fit.mle.sig_h4_ahead$f[4] # 12.96253 ($426,142.89)
fit.mle.sig_h4_ahead$ll[4] # 12.86814 ($387,759.27)
fit.mle.sig_h4_ahead$ul[4] # 13.05691 ($468,321.36)
fit.mle.sig_h20_ahead$f[20] # 13.18 ($529,665.00)
fit.mle.sig_h20_ahead$ll[20] # 12.94574 ($419,047.69)
fit.mle.sig_h20_ahead$ul[20] # 13.41426 ($669,482.30)

# Forecasting next year (Zoomed In)
plot(seq(xmin_plot,l+h.short,1),x[xmin_plot:(l+h.short)],type="l",col="black",
     xlab="Time", ylab = "log Median Housing Sales Price",
     main="Linear Signal with AR(6) Noise Short Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.short),1),fit.mle.sig_h4_ahead$f,col="red")
lines(seq((l+1),(l+h.short),1),fit.mle.sig_h4_ahead$ll,col="blue",lty=3) # lty=3 for dotted line
lines(seq((l+1),(l+h.short),1),fit.mle.sig_h4_ahead$ul,col="blue",lty=3)

# Forecasting next 5 years (Zoomed In)
plot(seq(xmin_plot,l+h.long,1),x[xmin_plot:(l+h.long)],type="l",col="black",
     xlab="Time", ylab = "log Median Housing Sales Price",
     main="Linear Signal with AR(6) Noise Long Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.long),1),fit.mle.sig_h20_ahead$f,col="red")
lines(seq((l+1),(l+h.long),1),fit.mle.sig_h20_ahead$ll,col="blue",lty=3) # lty=3 for dotted line
lines(seq((l+1),(l+h.long),1),fit.mle.sig_h20_ahead$ul,col="blue",lty=3)

```


# Multi-variate Models

We will try out an MLR model with lag and a VAR model that predicts all variables at the same time.  The exogenous predictions we made previously will be used for the MLR Model.

## MLR Model

We will first start by looking at the lag plots for the four exogenous variables.

```{r MLR lag plots}
x = fed_housing_data
l = ccf(x$Median_Sales_Price,x$Ownership_Rate,lag.max=80)
which.max(l$acf)
l$acf[20] # 0.3090685
l$lag[20] # 0
l = ccf(x$Median_Sales_Price,x$Housing_Units_Completed,lag.max=80)
which.max(l$acf)
l$acf[150] # 0.263895
l$lag[150] # 69, 21 seemed alright
ccf(x$Median_Sales_Price,x$Supply_New_Houses,main="Median Sales Price & Supply of New Houses")
l = ccf(x$Median_Sales_Price,x$Supply_New_Houses,lag.max=60)
which.min(l$acf)
l$acf[70] # -0.2592145
l$lag[70] # 9
l = ccf(x$Median_Sales_Price,x$Housing_Price_Index,lag.max=60)
which.max(l$acf)
l$acf[61] # 0.933195
l$lag[61] # 0
```

It looks like Ownership Rate has a lag of 0, Housing Units Completed has a lag of 21, Supply of New Houses has a lag of 9, and Housing Price Index has a lag of 0.

What does a model with these 4 lagged variables look like.

```{r MLR 4 lagged variables}
x.short = fed_housing_data_short
x.long = fed_housing_data_long
x$Year_Quarter = c()
x.short$Year_Quarter = c()
x.long$Year_Quarter = c()
l = length(x$Median_Sales_Price)
t=1:l
t.train.short= 1:(l-h.short)
t.test.short=(l-h.short+1):l
t.train.long= 1:(l-h.long)
t.test.long=(l-h.long+1):l
x$Housing_Units_Completed_l21 = dplyr::lag(x$Housing_Units_Completed,21)
x$Supply_New_Houses_l9 = dplyr::lag(x$Supply_New_Houses,9)
x.short$Housing_Units_Completed_l21 = dplyr::lag(x.short$Housing_Units_Completed,21)
x.short$Supply_New_Houses_l9 = dplyr::lag(x.short$Supply_New_Houses,9)
x.long$Housing_Units_Completed_l21 = dplyr::lag(x.long$Housing_Units_Completed,21)
x.long$Supply_New_Houses_l9 = dplyr::lag(x.long$Supply_New_Houses,9)
ksfit = lm(x$Median_Sales_Price~x$Ownership_Rate+x$Housing_Units_Completed_l21+x$Supply_New_Houses_l9+x$Housing_Price_Index+t)
summary(ksfit)
AIC(ksfit) # -454.2937
```

The Ownership rate variable didn't have a very high p value, so it wasn't significant in the model.  Let's see what happens when it is removed.

```{r MLR 3 lagged variables}
ksfit = lm(x$Median_Sales_Price~x$Housing_Units_Completed_l21+x$Supply_New_Houses_l9+x$Housing_Price_Index+t)
summary(ksfit)
AIC(ksfit) # -456.0482
```

In this model, all the variables are significant.  The AIC value is slightly lower as well.  We will now determine how to model the residuals using an ARMA model.

```{r MLR ARMA}
aic5.wge(ksfit$residuals,p=0:4,q=0:2,type='aic') # best 2/0
```

The model with the best AIC is an AR(2).

```{r MLR ASEs}
fit=arima(x.short$Median_Sales_Price[t.train.short],order=c(2,0,0),xreg=cbind(t.train.short,x.short$Housing_Units_Completed_l21[t.train.short],x.short$Supply_New_Houses_l9[t.train.short],x.short$Housing_Price_Index[t.train.short]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.short$Housing_Units_Completed_l21[t.test.short],Supply_New_Houses_l9=x.short$Supply_New_Houses_l9[t.test.short],Housing_Price_Index=x.short$Housing_Price_Index[t.test.short]))
ase = mean((fed_housing_data_NL$Median_Sales_Price[t.test.short]-exp(preds$pred))^2)/1e6
ase # 588.675
plot(seq(1,l,1),x.short$Median_Sales_Price,type="b")
points(seq((l-h.short+1),l,1),preds$pred,type="b",pch=15,col="blue")
fit=arima(x.long$Median_Sales_Price[t.train.long],order=c(2,0,0),xreg=cbind(t.train.long,x$Housing_Units_Completed_l21[t.train.long],x$Supply_New_Houses_l9[t.train.long],x$Housing_Price_Index[t.train.long]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.long$Housing_Units_Completed_l21[t.test.long],Supply_New_Houses_l9=x.long$Supply_New_Houses_l9[t.test.long],Housing_Price_Index=x.long$Housing_Price_Index[t.test.long]))
ase = mean((fed_housing_data_NL$Median_Sales_Price[t.test.long]-exp(preds$pred))^2)/1e6
ase # 1431.873
plot(seq(1,l,1),x.long$Median_Sales_Price,type="b")
points(seq((l-h.long+1),l,1),preds$pred,type="b",pch=15,col="blue")
```

```{r MLR Plots for PPT}
fit=arima(x.short$Median_Sales_Price[t.train.short],order=c(2,0,0),xreg=cbind(t.train.short,x.short$Housing_Units_Completed_l21[t.train.short],x.short$Supply_New_Houses_l9[t.train.short],x.short$Housing_Price_Index[t.train.short]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.short$Housing_Units_Completed_l21[t.test.short],Supply_New_Houses_l9=x.short$Supply_New_Houses_l9[t.test.short],Housing_Price_Index=x.short$Housing_Price_Index[t.test.short]))
plot(seq(xmin_plot,l,1),x.short$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="MLR Short Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.short+1),l,1),preds$pred,col="red")
fit=arima(x.long$Median_Sales_Price[t.train.long],order=c(2,0,0),xreg=cbind(t.train.long,x$Housing_Units_Completed_l21[t.train.long],x$Supply_New_Houses_l9[t.train.long],x$Housing_Price_Index[t.train.long]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.long$Housing_Units_Completed_l21[t.test.long],Supply_New_Houses_l9=x.long$Supply_New_Houses_l9[t.test.long],Housing_Price_Index=x.long$Housing_Price_Index[t.test.long]))
plot(seq(xmin_plot,l,1),x.long$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="MLR Long Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.long+1),l,1),preds$pred,col="red")
```

The short term ASE is 589 M and the long term ASE is 1.4 B.

In order to forecast out past the current date, we had to make forecasts for the exogenous variables as well.

```{r MLR future forecasts of exogenous variables}
x = fed_housing_data
x$Year_Quarter = c()
fore_df <- as.data.frame(matrix(0, nrow = h.long, ncol = ncol(x)))
colnames(fore_df) <- colnames(x)

# Ownership Rate
x = fed_housing_data$Ownership_Rate 
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=FALSE,plot=FALSE)
fore_df$Ownership_Rate[1:h.long] = f$f

# Housing Units Complete
x = fed_housing_data$Housing_Units_Completed  
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=FALSE,plot=FALSE)
fore_df$Housing_Units_Completed[1:h.long] = f$f

# Housing Price Index
x = fed_housing_data$Housing_Price_Index 
d = artrans.wge(x,1)
d2 = artrans.wge(d,1)
d3 = artrans.wge(d2,c(0,-1))
dev.off()
est = est.arma.wge(d3,p=6,q=5)
m = mult.wge(fac1=est$phi,fac2=c(0,-1))
f = fore.arima.wge(x,d=2,phi=m$model.coef,theta=est$theta,n.ahead=h.long,lastn=FALSE, plot=FALSE)
fore_df$Housing_Price_Index[1:h.long] = f$f

# Supply of New Houses
x = fed_housing_data$Supply_New_Houses   
est = est.arma.wge(x,p=1,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=FALSE, plot=FALSE)
fore_df$Supply_New_Houses[1:h.long] = f$f

# Forecast df
x = fed_housing_data
x$Year_Quarter = c()
fed_housing_data_forecast <- rbind(x, fore_df)
```

```{r MLR future forecasts}
x = fed_housing_data_forecast
x$Housing_Units_Completed_l21 = dplyr::lag(x$Housing_Units_Completed,21)
x$Supply_New_Houses_l9 = dplyr::lag(x$Supply_New_Houses,9)
t=1:l
t.fore.short = (l+1):(l+h.short)
t.fore.long = (l+1):(l+h.long)
fit=arima(x$Median_Sales_Price[t],order=c(2,0,0),xreg=cbind(t,x$Housing_Units_Completed_l21[t],x$Supply_New_Houses_l9[t],x$Housing_Price_Index[t]))
preds = predict(fit,newxreg = data.frame(t=t.fore.short,Housing_Units_Completed_l21=x$Housing_Units_Completed_l21[t.fore.short],Supply_New_Houses_l9=x$Supply_New_Houses_l9[t.fore.short],Housing_Price_Index=x$Housing_Price_Index[t.fore.short]))
plot(seq(xmin_plot,l+h.short,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.short)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="MLR Short Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.short),1),preds$pred,col="red")
preds = predict(fit,newxreg = data.frame(t=t.fore.long,Housing_Units_Completed_l21=x$Housing_Units_Completed_l21[t.fore.long],Supply_New_Houses_l9=x$Supply_New_Houses_l9[t.fore.long],Housing_Price_Index=x$Housing_Price_Index[t.fore.long]))
plot(seq(xmin_plot,l+h.long,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.long)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="MLR Long Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.long),1),preds$pred,col="red")
```

The MLR forecasts continued to increase with time.

## VAR Model

```{r VAR AIC}
x = fed_housing_data
x$Year_Quarter = c()
VARselect(x,lag.max=16,type="both",season=NULL,exogen=NULL) # lag = 4
```

The model with lag = 4 was chosen.   

```{r VAR ASEs}
fit = VAR(x,p=4,type='both') 
summary(fit) # trend and const were significant, but only lag up to 2 for variable of interest, huc not very predictive

fit = VAR(x[1:(l-h.short),],p=4,type='both') 
preds=predict(fit,n.ahead=h.short)
ase = mean((fed_housing_data_NL$Median_Sales_Price[(l-h.short+1):l]-exp(preds$fcst$Median_Sales_Price[,1]))^2)/1e6
ase # 43.63274
plot(seq(1,l,1),x$Median_Sales_Price,type="b")
points(seq(l-h.short+1,l,1),preds$fcst$Median_Sales_Price[1:h.short,1],type="b",pch=15,col="blue")
fanchart(preds)

fit = VAR(x[1:(l-h.long),],p=4,type='both') 
preds=predict(fit,n.ahead=h.long)
ase = mean((fed_housing_data_NL$Median_Sales_Price[(l-h.long+1):l]-exp(preds$fcst$Median_Sales_Price[,1]))^2)/1e6
ase # 3009.65
plot(seq(1,l,1),x$Median_Sales_Price,type="b")
points(seq(l-h.long+1,l,1),preds$fcst$Median_Sales_Price[1:h.long,1],type="b",pch=15,col="blue")
fanchart(preds)
```

```{r VAR PPT Plots}
x = fed_housing_data
x$Year_Quarter = c()
fit = VAR(x[1:(l-h.short),],p=4,type='both') 
preds=predict(fit,n.ahead=h.short)
plot(seq(xmin_plot,l,1),x$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="VAR Short Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.short+1),l,1),preds$fcst$Median_Sales_Price[,1],col="red")
lines(seq((l-h.short+1),l,1),preds$fcst$Median_Sales_Price[,2],col="blue",lty=3) # 2nd column is lower bound
lines(seq((l-h.short+1),l,1),preds$fcst$Median_Sales_Price[,3],col="blue",lty=3) # 3rd column is upper bound
fit = VAR(x[1:(l-h.long),],p=4,type='both')
preds=predict(fit,n.ahead=h.long)
plot(seq(xmin_plot,l,1),x$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="VAR Long Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.long+1),l,1),preds$fcst$Median_Sales_Price[,1],col="red")
lines(seq((l-h.long+1),l,1),preds$fcst$Median_Sales_Price[,2],col="blue",lty=3)
lines(seq((l-h.long+1),l,1),preds$fcst$Median_Sales_Price[,3],col="blue",lty=3)
```

The VAR model has a very small short term ASE of 46.4 M.  This is even smaller than the signal plus noise model.  However, its long term ASE is larger than the MLR model.

We forcasted out the VAR model for 1 year and 5 years. 

```{r VAR future forecasts}
x = fed_housing_data
x$Year_Quarter = c()
fit = VAR(x,p=4,type='both') 
preds=predict(fit,n.ahead=h.short)
plot(seq(xmin_plot,l+h.short,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.short)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="VAR Short Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l+1),(l+h.short),1),preds$fcst$Median_Sales_Price[,1],col="red")
lines(seq((l+1),(l+h.short),1),preds$fcst$Median_Sales_Price[,2],col="blue",lty=3) # 2nd column is lower bound
lines(seq((l+1),(l+h.short),1),preds$fcst$Median_Sales_Price[,3],col="blue",lty=3) # 3rd column is upper bound
fit = VAR(x,p=4,type='both')
preds=predict(fit,n.ahead=h.long)
plot(seq(xmin_plot,l+h.long,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.long)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="VAR Long Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l+1),(l+h.long),1),preds$fcst$Median_Sales_Price[,1],col="red")
lines(seq((l+1),(l+h.long),1),preds$fcst$Median_Sales_Price[,2],col="blue",lty=3)
lines(seq((l+1),(l+h.long),1),preds$fcst$Median_Sales_Price[,3],col="blue",lty=3)

plot(seq(xmin_plot,l+h.long,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.long)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="MLR Long Term Forecast",ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.long),1),preds$pred,col="red")
```

# Neural Networks

We investigated 3 Neural Nets for predictions: Multilayer Perceptron (MLP), Long Short-Term Memory(LSTM), and Temporal Fusion Transformer (TFT).

## Multilayer Perceptron

Four different univariate MLP models were fit to the data for both horizons, and the ASE and RMSE were best in both cases using MLP's default parameters. For the 1 year horizon, the function chose 5 hidden nodes and a Difference of 1. The univariate lags were 1, 2, 3, and 4. For the 5 year horizon, the function chose 5 hidden nodes and a Difference of 1 as well, with lags at 1, 3, and 4. The univariate MLP was used as a baseline for the Multivariate MLP.

```{r MLP Univariate}
# Univariate
log.mhp = fed_housing_data$Median_Sales_Price
msp.194 = ts(log.mhp[1:194])
msp.178 = ts(log.mhp[1:178])

# 1 Year Horizons
mspFit.4 = mlp(msp.194, comb = 'median')
plot(mspFit.4)
mspFit.d0.4 = mlp(msp.194, comb = 'median', difforder = 0)
mspFit.d1.4 = mlp(msp.194, comb = 'median', difforder = 1)
mspFit.d2.4 = mlp(msp.194, comb = 'median', difforder = 2)

f.4 = forecast(mspFit.4, h = 4)
f.d0.4 = forecast(mspFit.d0.4, h = 4)
f.d1.4 = forecast(mspFit.d1.4, h = 4)
f.d2.4 = forecast(mspFit.d2.4, h = 4)

# Defaults
ASE.mlp.exp.h4 = mean((exp(log.mhp[195:198]) - exp(f.4$mean))^2) # 60.95M
rwfit.mlp4 = roll.win.rmse.nn.wge(log.mhp, horizon = 4, fit_model = mspFit.4) # 0.029

# Difforder = 0
ASE.mlp.exp.d0.h4 = mean((exp(log.mhp[195:198]) - exp(f.d0.4$mean))^2) # 99.75M
rwfit.mlp4.d0 = roll.win.rmse.nn.wge(log.mhp, horizon = 4, fit_model = mspFit.d0.4) # 0.033

# Difforder = 1
ASE.mlp.exp.d1.h4 = mean((exp(log.mhp[195:198]) - exp(f.d1.4$mean))^2) # 178.5M
rwfit.mlp4.d1 = roll.win.rmse.nn.wge(log.mhp, horizon = 4, fit_model = mspFit.d1.4) # 0.029

# Difforder = 2
ASE.mlp.exp.d2.h4 = mean((exp(log.mhp[195:198]) - exp(f.d2.4$mean))^2) # 205.4M
rwfit.mlp4.d2 = roll.win.rmse.nn.wge(log.mhp, horizon = 4, fit_model = mspFit.d2.4) # 0.031

# 1 Year Horizon
plot(log.mhp[195:198], type = 'l', ylim = c(12.9, 13))
lines(seq(1,4), f.4$mean, col = "blue")
lines(seq(1,4), f.d0.4$mean, col = "red")
lines(seq(1,4), f.d1.4$mean, col = "orange")
lines(seq(1,4), f.d2.4$mean, col = "green")

# 5 Year Horizon
mspFit.20 = mlp(msp.178, comb = 'median')
plot(mspFit.20)
mspFit.d0.20 = mlp(msp.178, comb = 'median', difforder = 0)
mspFit.d1.20 = mlp(msp.178, comb = 'median', difforder = 1)
mspFit.d2.20 = mlp(msp.178, comb = 'median', difforder = 2)

f.20 = forecast(mspFit.20, h = 20)
plot(f.20)
f.d0.20 = forecast(mspFit.d0.20, h = 20)
f.d1.20 = forecast(mspFit.d1.20, h = 20)
f.d2.20 = forecast(mspFit.d2.20, h = 20)

# Defaults
ASE.mlp.exp.h20 = mean((exp(log.mhp[179:198]) - exp(f.20$mean))^2) # 1.72B
rwfit.mlp20 = roll.win.rmse.nn.wge(log.mhp, horizon = 20, fit_model = mspFit.20) # 0.072

# Difforder = 0
ASE.mlp.exp.d0.h20 = mean((exp(log.mhp[179:198]) - exp(f.d0.20$mean))^2) # 7.24B
rwfit.mlp20.d0 = roll.win.rmse.nn.wge(log.mhp, horizon = 20, fit_model = mspFit.d0.20) # 0.076

# Difforder = 1
ASE.mlp.exp.d1.h20 = mean((exp(log.mhp[179:198]) - exp(f.d1.20$mean))^2) # 2.02B
rwfit.mlp20.d1 = roll.win.rmse.nn.wge(log.mhp, horizon = 20, fit_model = mspFit.d1.20) # 0.073

# Difforder = 2
ASE.mlp.exp.d2.h20 = mean((exp(log.mhp[179:198]) - exp(f.d2.20$mean))^2) # 2.61B
rwfit.mlp20.d2 = roll.win.rmse.nn.wge(log.mhp, horizon = 20, fit_model = mspFit.d2.20) # 0.077

# Create table for comparison
labels = c("Defaults", "DiffOrder = 0", "DiffOrder = 1", "DiffOrder = 2")
ASE_4 = c(ASE.mlp.exp.h4, ASE.mlp.exp.d0.h4, ASE.mlp.exp.d1.h4, ASE.mlp.exp.d2.h4)
ASE_20 = c(ASE.mlp.exp.h20, ASE.mlp.exp.d0.h20, ASE.mlp.exp.d1.h20, ASE.mlp.exp.d2.h20)
RMSE_4 = c(rwfit.mlp4$rwRMSE,rwfit.mlp4.d0$rwRMSE,rwfit.mlp4.d1$rwRMSE,rwfit.mlp4.d2$rwRMSE) 
RMSE_20 = c(rwfit.mlp20$rwRMSE,rwfit.mlp20.d0$rwRMSE,rwfit.mlp20.d1$rwRMSE,rwfit.mlp20.d2$rwRMSE) 
comparison = data.frame(Metrics = labels, ASE_1_yr = ASE_4, ASE_5_Yr = ASE_20, RMSE_1_Yr = RMSE_4, RMSE_5_Yr = RMSE_20)
comparison

# Comparing 5 Year Forecasts
plot(log.mhp[179:198], type = 'l', ylim = c(12.5, 13))
lines(seq(1,20), f.20$mean, col = "blue")
lines(seq(1,20), f.d0.20$mean, col = "red")
lines(seq(1,20), f.d1.20$mean, col = "orange")
lines(seq(1,20), f.d2.20$mean, col = "green")

# Forecasts
plot(log.mhp, type = 'l', lwd = 1, main = 'Univariate Forecasts, 1 and 5 Years')
points(seq(195,198), f.4$mean, type = 'l', col = 'red', lwd = 2)
points(seq(179,198), f.20$mean, type = 'l', col = 'red', lwd = 2)

# Zoomed In, 1 yr
plot(seq(150,198,1),log.mhp[150:198], type = 'l', lwd = 1, main = 'Univariate MLP Forecast, 1 Year')
points(seq(195,198,1), f.4$mean, type = 'l', col = 'red', lwd = 2)

# Zoomed In, 5 yr
plot(seq(150,198,1), log.mhp[149:198], type = 'l', lwd = 1, main = 'Univariate MLP Forecast, 5 Years')
points(seq(179,198,1), f.20$mean, type = 'l', col = 'red', lwd = 2)
```

## Confidence Intervals for Univariate Model

We were able to create confidence intervals for the univariate model by taking the 5th and 95th percentiles of bootstrapped residuals. These confidence intervals did contain the actual values for both backcasts.

```{r Univariate MLP Confidence Intervals}
################################################################################

# Trying what Dr Sadler did, but for an mlp model

# xt = gen.arma.wge(100,phi = .9)
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSPUS.csv"
mhp <- read.csv(file_path, header = TRUE)
mhp_1975 = mhp[49:246,]
xt = log(mhp_1975$MSPUS)

# Horizon = 4
h.long = 4
l = length(xt)
xtTrain = xt[1:(l-h.long)]

# est = est.arma.wge(xtTrain,p = 1)
library(nnfor)
set.seed(30)
fit.mlp = mlp(ts(xt), comb = 'median')
lf = length(fit.mlp$fitted)
res = xt[(l-lf+1):l]-fit.mlp$fitted

# Are residuals white noise?
plotts.sample.wge(res)
acf(res,lag.max=100)
ljung.wge(res,K=24) # p = 0.3804433, white noise
ljung.wge(res,K=48) # p = 0.6786805, white
hist(res) # look normally distributed

# BSSim = function(ts,res,phi,horizon) {
num_back = l - lf
BSSim = function(ts,res,fit,num_back,horizon) {
  origin = length(ts)
  ForHolder = numeric(horizon)
  for(i in (origin+1):(origin+horizon)){
    # ts[i] = ts[i-1]*phi + sample(res,1)
    # print(i)
    needed_to_predict = ts[(i-num_back):(i-1)]
    fore=forecast(fit,y=needed_to_predict,h=1)
    ts[i]=fore$mean+sample(res,1)
    ForHolder[(i-origin)]=ts[i]
  }
  return(ForHolder)
}

# BSSamples = 100000
BSSamples = 1000 # I'm worried 100k will take a really long time, so make it less

# holder = matrix(nrow = BSSamples,ncol = 50)
holder = matrix(nrow = BSSamples,ncol = h.long)

for(i in 1:BSSamples)
{
  # xtNewFor = BSSim(xtTrain,est$res,est$phi,50)
  if ((i %% 100) == 0){
    print(i)
  }
  xtNewFor = BSSim(xtTrain,res,fit.mlp,num_back,h.long)
  holder[i,] = xtNewFor
}

# Calculate percentiles for each column
percentiles_per_column <- apply(holder, 2, function(column) {
  quantile(column, probs = c(0.025, 0.975))
})

# Transpose the result for better readability
percentiles_per_column <- t(percentiles_per_column)

# Print the result
print(percentiles_per_column)

xtNewForBS = colMeans(holder)

result = fit.mlp$fitted

# plotts.wge(c(xtTrain-est$res,xtNewForBS))
#plotts.wge(c(fit.mlp$fitted,xtNewForBS),ylim=c(min(fit.mlp$fitted)-.1,max(fit.mlp$fitted)+.1), )
#lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,1],col = "blue")
#lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,2],col = "blue")
#lines(seq(lf+1,lf+h.long,1),xt[(l-h.long+1):l],col = "red")

# Alex edited seq because lines were being plotted that didn't line up with the actual time
# plotts.wge(c(xtTrain-est$res,xtNewForBS))
plot(log.mhp[6:198], type = 'l', main = 'Univariate MLP Short Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price', ylim=c(10.5,13.1))
lines(seq(190,193,1),percentiles_per_column[,1], lty = 3, col = "blue")
lines(seq(190,193,1),percentiles_per_column[,2], lty = 3, col = "blue")
lines(seq(190,193,1),xtNewForBS, col = "red")

# Zoomed In
plot(seq(150,198,1), log.mhp[150:198], type = 'l', main = 'Univariate MLP Short Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price', ylim=c(12.4,13.1))
lines(seq(195,198,1),percentiles_per_column[,1], lty = 3, col = "blue")
lines(seq(195,198,1),percentiles_per_column[,2], lty = 3, col = "blue")
lines(seq(195,198,1),xtNewForBS,ylim=c(min(fit.mlp$fitted)-.1,max(fit.mlp$fitted)+.1),col = "red")

# Horizon = 20  
h.long = 20
l = length(xt)
xtTrain = xt[1:(l-h.long)]

# est = est.arma.wge(xtTrain,p = 1)
library(nnfor)
set.seed(30)
fit.mlp = mlp(ts(xt), comb = 'median')
lf = length(fit.mlp$fitted)
res = xt[(l-lf+1):l]-fit.mlp$fitted

# Are residuals white noise?
plotts.sample.wge(res)
acf(res,lag.max=100)
ljung.wge(res,K=24) # p = 0.3804433, white noise
ljung.wge(res,K=48) # p = 0.6786805, white
hist(res) # look normally distributed

# BSSim = function(ts,res,phi,horizon) {
num_back = l - lf
BSSim = function(ts,res,fit,num_back,horizon) {
  origin = length(ts)
  ForHolder = numeric(horizon)
  for(i in (origin+1):(origin+horizon)){
    # ts[i] = ts[i-1]*phi + sample(res,1)
    # print(i)
    needed_to_predict = ts[(i-num_back):(i-1)]
    fore=forecast(fit,y=needed_to_predict,h=1)
    ts[i]=fore$mean+sample(res,1)
    ForHolder[(i-origin)]=ts[i]
  }
  return(ForHolder)
}

# BSSamples = 100000
BSSamples = 1000 # I'm worried 100k will take a really long time, so make it less

# holder = matrix(nrow = BSSamples,ncol = 50)
holder = matrix(nrow = BSSamples,ncol = h.long)

for(i in 1:BSSamples)
{
  # xtNewFor = BSSim(xtTrain,est$res,est$phi,50)
  if ((i %% 100) == 0){
    print(i)
  }
  xtNewFor = BSSim(xtTrain,res,fit.mlp,num_back,h.long)
  holder[i,] = xtNewFor
}

# Calculate percentiles for each column
percentiles_per_column <- apply(holder, 2, function(column) {
  quantile(column, probs = c(0.025, 0.975))
})

# Transpose the result for better readability
percentiles_per_column <- t(percentiles_per_column)

# Print the result
print(percentiles_per_column)

xtNewForBS = colMeans(holder)

# plotts.wge(c(xtTrain-est$res,xtNewForBS))
#plotts.wge(c(fit.mlp$fitted,xtNewForBS),ylim=c(min(fit.mlp$fitted)-.1,max(fit.mlp$fitted)+.1), )
#lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,1],col = "blue")
#lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,2],col = "blue")
#lines(seq(lf+1,lf+h.long,1),xt[(l-h.long+1):l],col = "red")

# Alex edited seq because lines were being plotted that didn't line up with the actual time
# plotts.wge(c(xtTrain-est$res,xtNewForBS))
plot(log.mhp[6:198], type = 'l', main = 'Univariate MLP Long Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price', ylim=c(10.5,13.1))
lines(seq(174,193,1),percentiles_per_column[,1], lty = 3, col = "blue")
lines(seq(174,193,1),percentiles_per_column[,2], lty = 3, col = "blue")
lines(seq(174,193,1),xtNewForBS,col = "red")

# Zoomed In
plot(seq(150,198,1),log.mhp[150:198], type = 'l', main = 'Univariate MLP Long Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price', ylim=c(12.4,13.1))
lines(seq(179,198,1),percentiles_per_column[,1], lty = 3, col = "blue")
lines(seq(179,198,1),percentiles_per_column[,2], lty = 3, col = "blue")
lines(seq(179,198,1),xtNewForBS, col = "red")



```

## Multivariate MLP

We used forecasted values from the default MLP parameters for our exogenous variables in order to fit a multivariate model to our response variable. The 

Four different Multivariate MLP models were fit to the data for both horizons. For the horizon of 1 year, and the ASE and RMSE were best using an MLP with a Difference of 1. For the 1 year horizon, the function chose 5 hidden nodes. The univariate lag was 1, with lags at 3 & 4 for Home Ownership Rate (Regressor 1), lags 1 & 2 for Housing Units Completed (Regressor 2), and lags at 1, 3, & 4 for Supply of New Houses (Regressor 3). No lags were chosen for the Housing Price Index (Regressor 4). For the 5 year horizon, and the ASE and RMSE were best using an MLP with a Difference of 2. The MLP function chose 5 hidden nodes, with univariate lags at 1, 2, and 3. It included lays at 1 & 2 for Home Ownership Rate (Regressor 1) and at 1, 3, and 4 for Housing Units Completed (Regressor 2). There were no lags chosen for Supply of New Houses (Regressor 3) or Housing Price Index (Regressor 4). We noted that the ASEs and RMSEs for the best Multivariate MLP model were not better than the best univariate MLP model.

```{r MLP Multivariate Baseline}
# Multivariate

# Establishing Baseline

# Response Variable
mhp.train.4 = ts(fed_housing_data$Median_Sales_Price[1:194])
mhp.train.20 = ts(fed_housing_data$Median_Sales_Price[1:178])

mhp.test.4 = fed_housing_data$Median_Sales_Price[195:198]
mhp.test.20 = fed_housing_data$Median_Sales_Price[179:198]

# Exogenous Variables
hor.194 = fed_housing_data$Ownership_Rate[1:194]
hor.178 = fed_housing_data$Ownership_Rate[1:178]

huc.194 = fed_housing_data$Housing_Units_Completed[1:194]
huc.178 = fed_housing_data$Housing_Units_Completed[1:178]

snh.194 = fed_housing_data$Supply_New_Houses[1:194]
snh.178 = fed_housing_data$Supply_New_Houses[1:178]

hpi.194 = fed_housing_data$Housing_Price_Index[1:194]
hpi.178 = fed_housing_data$Housing_Price_Index[1:178]

# Exogenous Variable Training Dataframe for xreg
ts.train.X.4 = data.frame(hor = ts(hor.194), huc = ts(huc.194), snh = ts(snh.194), hpi = ts(hpi.194))
ts.train.X.20 = data.frame(hor = ts(hor.178), huc = ts(huc.178), snh = ts(snh.178), hpi = ts(hpi.178))

# Baseline forecasts for 1 yr
fit.mlp.hor.4 = mlp(ts(ts.train.X.4$hor), reps = 30, comb = 'median')
fore.mlp.hor.4 = forecast(fit.mlp.hor.4, h = 4)

fit.mlp.huc.4 = mlp(ts(ts.train.X.4$huc), reps = 30, comb = 'median')
fore.mlp.huc.4 = forecast(fit.mlp.huc.4, h = 4)

fit.mlp.snh.4 = mlp(ts(ts.train.X.4$snh), reps = 30, comb = 'median')
fore.mlp.snh.4 = forecast(fit.mlp.snh.4, h = 4)

fit.mlp.hpi.4 = mlp(ts(ts.train.X.4$hpi), reps = 30, comb = 'median')
fore.mlp.hpi.4 = forecast(fit.mlp.hpi.4, h = 4)

# Baseline forecasts for 5 yr
fit.mlp.hor.20 = mlp(ts(ts.train.X.20$hor), reps = 30, comb = 'median')
fore.mlp.hor.20 = forecast(fit.mlp.hor.20, h = 20)

fit.mlp.huc.20 = mlp(ts(ts.train.X.20$huc), reps = 30, comb = 'median')
fore.mlp.huc.20 = forecast(fit.mlp.huc.20, h = 20)

fit.mlp.snh.20 = mlp(ts(ts.train.X.20$snh), reps = 30, comb = 'median')
fore.mlp.snh.20 = forecast(fit.mlp.snh.20, h = 20)

fit.mlp.hpi.20 = mlp(ts(ts.train.X.20$hpi), reps = 30, comb = 'median')
fore.mlp.hpi.20 = forecast(fit.mlp.hpi.20, h = 20)

# Exogenous Variable Testing Dataframe for xreg
ts.test.X.4 = data.frame(hor = ts(fore.mlp.hor.4$mean), huc = ts(fore.mlp.huc.4$mean), 
                         snh = ts(fore.mlp.snh.4$mean), hpi = ts(fore.mlp.hpi.4$mean))

ts.test.X.20 = data.frame(hor = ts(fore.mlp.hor.20$mean), huc = ts(fore.mlp.huc.20$mean), 
                          snh = ts(fore.mlp.snh.20$mean), hpi = ts(fore.mlp.hpi.20$mean))

# Known and Forecasted Dataframe for xreg
ts.forecasted.X.4 = data.frame(hor = ts(c(hor.194, fore.mlp.hor.4$mean)), huc = ts(c(huc.194, fore.mlp.huc.4$mean)), 
                         snh = ts(c(snh.194, fore.mlp.snh.4$mean)), hpi = ts(c(hpi.194, fore.mlp.hpi.4$mean)))

ts.forecasted.X.20 = data.frame(hor = ts(c(hor.178, fore.mlp.hor.20$mean)), huc = ts(c(huc.178, fore.mlp.huc.20$mean)), 
                         snh = ts(c(snh.178, fore.mlp.snh.20$mean)), hpi = ts(c(hpi.178, fore.mlp.hpi.20$mean)))

# Default Fits for 1 yr and 5 yr
fit.mlp.x.4 = mlp(mhp.train.4, reps = 30, comb = 'median', xreg = ts.train.X.4, sel.lag = TRUE)
fit.mlp.x.20 = mlp(mhp.train.20, reps = 30, comb = 'median', xreg = ts.train.X.20, difforder = 0, sel.lag = TRUE)

# Default Forecasting the last 1 yr & 5 yrs
forecasted.mlp.X.4 = forecast(fit.mlp.x.4, h = 4, xreg = ts.forecasted.X.4)
forecasted.mlp.X.20 = forecast(fit.mlp.x.20, h = 20, xreg = ts.forecasted.X.20)

# Difforder = 0
fit.mlp.x.d0.4 = mlp(mhp.train.4, reps = 30, comb = 'median', xreg = ts.train.X.4, difforder = 0, sel.lag = TRUE)
fit.mlp.x.d0.20 = mlp(mhp.train.20, reps = 30, comb = 'median', xreg = ts.train.X.20, difforder = 0, sel.lag = TRUE)

# Difforder 0 Forecasting the last 1 yr & 5 yrs
forecasted.mlp.X.d0.4 = forecast(fit.mlp.x.d0.4, h = 4, xreg = ts.forecasted.X.4)
forecasted.mlp.X.d0.20 = forecast(fit.mlp.x.d0.20, h = 20, xreg = ts.forecasted.X.20)

# Diff Order = 1
fit.mlp.x.d1.4 = mlp(mhp.train.4, reps = 30, comb = 'median', xreg = ts.train.X.4, difforder = 1, sel.lag = TRUE)
fit.mlp.x.d1.20 = mlp(mhp.train.20, reps = 30, comb = 'median', xreg = ts.train.X.20, difforder = 1, sel.lag = TRUE)

# Difforder 1 Forecasting the last 1 yr & 5 yrs
forecasted.mlp.X.d1.4 = forecast(fit.mlp.x.d1.4, h = 4, xreg = ts.forecasted.X.4)
forecasted.mlp.X.d1.20 = forecast(fit.mlp.x.d1.20, h = 20, xreg = ts.forecasted.X.20)

# Diff Order = 2
fit.mlp.x.d2.4 = mlp(mhp.train.4, reps = 30, comb = 'median', xreg = ts.train.X.4, difforder = 2, sel.lag = TRUE)
fit.mlp.x.d2.20 = mlp(mhp.train.20, reps = 30, comb = 'median', xreg = ts.train.X.20, difforder = 2, sel.lag = TRUE)

# Difforder 2 Forecasting the last 1 yr & 5 yrs
forecasted.mlp.X.d2.4 = forecast(fit.mlp.x.d2.4, h = 4, xreg = ts.forecasted.X.4)
forecasted.mlp.X.d2.20 = forecast(fit.mlp.x.d2.20, h = 20, xreg = ts.forecasted.X.20)

# Models Chosen
fit.mlp.x.4
fit.mlp.x.d1.4

```



## Assessing the Models

```{r Assessing mlp models}
# 1 Year Models
plot(log.mhp[195:198], type = 'l', ylim = c(12.75, 13.2), lwd = 3, main = "Actual Values vs. Univariate and Multivariate MLP 1 Year forecasts")
lines(seq(1,4), f.4$mean, col = "purple")
lines(seq(1,4), forecasted.mlp.X.4$mean, col = "blue")
lines(seq(1,4), forecasted.mlp.X.d0.4$mean, col = "red")
lines(seq(1,4), forecasted.mlp.X.d1.4$mean, col = "orange")
lines(seq(1,4), forecasted.mlp.X.d2.4$mean, col = "green")

# Default Model
ASE.mlp.exp.x.h4 = mean((exp(log.mhp[195:198]) - exp(forecasted.mlp.X.4$mean))^2) # 429M
RMSE.baseline.x.4 = sqrt(mean((mhp.test.4 - forecasted.mlp.X.4$mean)^2)) # 0.048

# Difforder = 0 Model
ASE.mlp.exp.x.d0.h4 = mean((exp(log.mhp[195:198]) - exp(forecasted.mlp.X.d0.4$mean))^2) # 858M
RMSE.x.d0.4 = sqrt(mean((mhp.test.4 - forecasted.mlp.X.d0.4$mean)^2)) # 0.067

# Difforder = 1 Model
ASE.mlp.exp.x.d1.h4 = mean((exp(log.mhp[195:198]) - exp(forecasted.mlp.X.d1.4$mean))^2) #169M
RMSE.x.d1.4 = sqrt(mean((mhp.test.4 - forecasted.mlp.X.d1.4$mean)^2)) # 0.0305

# Difforder = 2 Model
ASE.mlp.exp.x.d2.h4 = mean((exp(log.mhp[195:198]) - exp(forecasted.mlp.X.d2.4$mean))^2) # 425M
RMSE.x.d2.4 = sqrt(mean((mhp.test.4 - forecasted.mlp.X.d2.4$mean)^2)) # 0.0478

# 5 Yr Models
plot(log.mhp[179:198], type = 'l', ylim = c(12.3, 13.1), lwd = 3, main = "Actual Values vs. Univariate and Multivariate MLP 5 Year forecasts")
lines(seq(1,20), f.20$mean, col = "purple")
lines(seq(1,20), forecasted.mlp.X.20$mean, col = "blue")
lines(seq(1,20), forecasted.mlp.X.d0.20$mean, col = "red")
lines(seq(1,20), forecasted.mlp.X.d1.20$mean, col = "orange")
lines(seq(1,20), forecasted.mlp.X.d2.20$mean, col = "green")

# Baseline Model
ASE.mlp.exp.x.h20 = mean((exp(log.mhp[179:198]) - exp(forecasted.mlp.X.20$mean))^2) #11.55B
RMSE.baseline.x.20 = sqrt(mean((mhp.test.20 - forecasted.mlp.X.20$mean)^2)) # 0.3062

# Diff Order 0 Model
ASE.mlp.exp.x.d0.h20 = mean((exp(log.mhp[179:198]) - exp(forecasted.mlp.X.d0.20$mean))^2) # 12.8B
RMSE.x.d0.20 = sqrt(mean((mhp.test.20 - forecasted.mlp.X.d0.20$mean)^2)) # 0.327

# Diff Order 1 Model
ASE.mlp.exp.x.d1.h20 = mean((exp(log.mhp[179:198]) - exp(forecasted.mlp.X.d1.20$mean))^2) # 14.86B
RMSE.x.d1.20 = sqrt(mean((mhp.test.20 - forecasted.mlp.X.d1.20$mean)^2)) # 0.3593

# Diff Order 2 Model
ASE.mlp.exp.x.d2.h20 = mean((exp(log.mhp[179:198]) - exp(forecasted.mlp.X.d2.20$mean))^2) # 8.437B
RMSE.x.d2.20 = sqrt(mean((mhp.test.20 - forecasted.mlp.X.d2.20$mean)^2)) # 0.253

# Create table for comparison
labels = c("Defaults", "DiffOrder = 0", "DiffOrder = 1", "DiffOrder = 2")
ASE_4 = c(ASE.mlp.exp.x.h4, ASE.mlp.exp.x.d0.h4, ASE.mlp.exp.x.d1.h4, ASE.mlp.exp.x.d2.h4)
ASE_20 = c(ASE.mlp.exp.x.h20, ASE.mlp.exp.x.d0.h20, ASE.mlp.exp.x.d1.h20, ASE.mlp.exp.x.d2.h20)
RMSE_4 = c(RMSE.baseline.x.4,RMSE.x.d0.4,RMSE.x.d1.4,RMSE.x.d2.4) 
RMSE_20 = c(RMSE.baseline.x.20,RMSE.x.d0.20,RMSE.x.d1.20,RMSE.x.d2.20) 
comparison = data.frame(Metrics = labels, ASE_1_yr = ASE_4, ASE_5_Yr = ASE_20, RMSE_1_Yr = RMSE_4, RMSE_5_Yr = RMSE_20)
comparison

# Best Multivariate Models
plot(log.mhp, type = 'l')
points(seq(195,198), forecasted.mlp.X.d1.4$mean, type = 'l', col = 'red', pch = 3)
points(seq(179,198), forecasted.mlp.X.d2.20$mean, type = 'l', col = 'red', pch = 3)

# Best Multivariate Models Zoomed In
plot(log.mhp[149:198], type = 'l', main = "Multivariate MLP Forecast for last 1 & 5 Years")
points(seq(47,50), forecasted.mlp.X.d1.4$mean, type = 'l', col = 'red', pch = 3)
points(seq(31,50), forecasted.mlp.X.d2.20$mean, type = 'l', col = 'red', pch = 3)

```
### Multivariate Forecasting

```{r Multivariate Forecasting}

# Refit the model using all the data
ts.X = data.frame(hor = ts(fed_housing_data$Ownership_Rate), huc = ts(fed_housing_data$Housing_Units_Completed), 
                  snh = ts(fed_housing_data$Supply_New_Houses), hpi = ts(fed_housing_data$Housing_Price_Index))

# Forecasting Exogenous Variables
fit.mlp.hor = mlp(ts(ts.X$hor), reps = 30, comb = 'median')
fore.mlp.hor.4 = forecast(fit.mlp.hor, h = 4)
fore.mlp.hor.20 = forecast(fit.mlp.hor, h = 20)

fit.mlp.huc = mlp(ts(ts.X$huc), reps = 30, comb = 'median')
fore.mlp.huc.4 = forecast(fit.mlp.huc, h = 4)
fore.mlp.huc.20 = forecast(fit.mlp.huc, h = 20)

fit.mlp.snh = mlp(ts(ts.X$snh), reps = 30, comb = 'median')
fore.mlp.snh.4 = forecast(fit.mlp.snh, h = 4)
fore.mlp.snh.20 = forecast(fit.mlp.snh, h = 20)

fit.mlp.hpi = mlp(ts(ts.X$hpi), reps = 30, comb = 'median')
fore.mlp.hpi.4 = forecast(fit.mlp.hpi, h = 4)
fore.mlp.hpi.20 = forecast(fit.mlp.hpi, h = 20)

# Known and Forecasted Dataframe for xreg
ts.all.X.4 = data.frame(hor = ts(c(ts.X$hor, fore.mlp.hor.4$mean)), huc = ts(c(ts.X$huc, fore.mlp.huc.4$mean)), 
                         snh = ts(c(ts.X$snh, fore.mlp.snh.4$mean)), hpi = ts(c(ts.X$hpi, fore.mlp.hpi.4$mean)))

ts.all.X.20 = data.frame(hor = ts(c(ts.X$hor, fore.mlp.hor.20$mean)), huc = ts(c(ts.X$hor, fore.mlp.huc.20$mean)), 
                         snh = ts(c(ts.X$hor, fore.mlp.snh.20$mean)), hpi = ts(c(ts.X$hor, fore.mlp.hpi.20$mean)))

# Difforder = 1 predictions, 1 Yr Horizon
mlpFit.xreg.d1.4 = mlp(ts(log.mhp), comb = 'median', difforder = 1, xreg = ts.X)
f.xreg.d1.4 = forecast(mlpFit.xreg.d1.4, h = 4, xreg = ts.all.X.4)

# Difforder = 2 predictions, 5 yr Horizon
mlpFit.xreg.d2.20 = mlp(ts(log.mhp), comb = 'median', difforder = 2, xreg = ts.X)
f.xreg.d2.20 = forecast(mlpFit.xreg.d2.20, h = 20, xreg = ts.all.X.20)

# Forecasting next year and 5 years
plot(log.mhp[1:198], type = 'l', xlim = c(1,218), ylim = c(10.5, 13.25))
lines(seq(199,202), f.xreg.d1.4$mean, col = "red")
lines(seq(199,218), f.xreg.d2.20$mean, col = "red")
```
## Confidence Intervals for Multivariate Models

Still working on getting code to work - 
Error: Error in newdata[, object$model.list$variables] : subscript out of bounds

```{r Multivariate MLP Confidence Intervals, include = FALSE}
################################################################################

# Trying what Dr Sadler did, but for an mlp model

# make dataframe
ts.X = data.frame(hor = ts(fed_housing_data$Ownership_Rate), huc = ts(fed_housing_data$Housing_Units_Completed), snh = ts(fed_housing_data$Supply_New_Houses), hpi = ts(fed_housing_data$Housing_Price_Index))

# Make an mlp
xt = fed_housing_data$Median_Sales_Price
fit = mlp(ts(xt), comb = 'median', xreg = ts.X, difforder = 1, sel.lag = TRUE) 

# Make the residuals
l = length(xt)
lf = length(fit$fitted)
res = xt[(l-lf+1):l]-fit$fitted
xtTrain = xt[1:(l-h.short)]
plotts.wge(res)

# Make the function
num_back = l - lf
BSSim = function(xtTrain,res,fit,num_back,horizon) {
  origin = length(xtTrain)
  ForHolder = numeric(horizon)
  for(i in (origin+1):(origin+horizon)){
    # ts[i] = ts[i-1]*phi + sample(res,1)
    # print(i)
    needed_to_predict = xtTrain[(i-num_back):(i-1)]
    needed_to_predict_xreg = ts.X[(i-num_back):i,] # You need an additional one foreward, since you need those to do the prediction for h = 1
    # fore=forecast(fit,y=needed_to_predict,h=1)
    fore=forecast(fit,y=needed_to_predict,h=1,xreg=needed_to_predict_xreg)
    xtTrain[i]=fore$mean+sample(res,1)
    ForHolder[(i-origin)]=xtTrain[i]
  }
  return(ForHolder)
}

# Run the function
BSSamples = 1000 # I'm worried 100k will take a really long time, so make it less
holder = matrix(nrow = BSSamples,ncol = h.short)
for(i in 1:BSSamples)
{
  # xtNewFor = BSSim(xtTrain,est$res,est$phi,50)
  if ((i %% 100) == 0){
    print(i)
  }
  xtNewFor = BSSim(xtTrain,res,fit,num_back,h.short)
  holder[i,] = xtNewFor
}

# Calculate percentiles for each column
percentiles_per_column <- apply(holder, 2, function(column) {
  quantile(column, probs = c(0.025, 0.975))
})

# Transpose the result for better readability
percentiles_per_column <- t(percentiles_per_column)

# Print the result
print(percentiles_per_column)

xtNewForBS = colMeans(holder)

# plotts.wge(c(xtTrain-est$res,xtNewForBS))
# plotts.wge(c(fit$fitted,xtNewForBS),ylim=c(min(fit$fitted)-.1,max(fit$fitted)+.1)) # black is predictions
plotts.wge(c(fit$fitted,xtNewForBS),ylim=c(12.4,max(fit$fitted)+.1),xlim=c(150,length(c(fit$fitted,xtNewForBS)))) # black is predictions
lines(seq(lf+1,lf+h.short,1),percentiles_per_column[,1],col = "blue") # blue are confidence intervals
lines(seq(lf+1,lf+h.short,1),percentiles_per_column[,2],col = "blue")
lines(seq(lf+1,lf+h.short,1),xt[(l-h.short+1):l],col = "red") # Actual values

# Horizon = 20

# Make the residuals
l = length(xt)
lf = length(fit$fitted)
res = xt[(l-lf+1):l]-fit$fitted
xtTrain = xt[1:(l-h.long)]
plotts.wge(res)

# Make the function
num_back = l - lf
BSSim = function(xtTrain,res,fit,num_back,horizon) {
  origin = length(xtTrain)
  ForHolder = numeric(horizon)
  for(i in (origin+1):(origin+horizon)){
    # ts[i] = ts[i-1]*phi + sample(res,1)
    # print(i)
    needed_to_predict = xtTrain[(i-num_back):(i-1)]
    needed_to_predict_xreg = ts.X[(i-num_back):i,] # You need an additional one foreward, since you need those to do the prediction for h = 1
    # fore=forecast(fit,y=needed_to_predict,h=1)
    fore=forecast(fit,y=needed_to_predict,h=1,xreg=needed_to_predict_xreg)
    xtTrain[i]=fore$mean+sample(res,1)
    ForHolder[(i-origin)]=xtTrain[i]
  }
  return(ForHolder)
}

# Run the function
BSSamples = 1000 # I'm worried 100k will take a really long time, so make it less
holder = matrix(nrow = BSSamples,ncol = h.long)
for(i in 1:BSSamples)
{
  # xtNewFor = BSSim(xtTrain,est$res,est$phi,50)
  if ((i %% 100) == 0){
    print(i)
  }
  xtNewFor = BSSim(xtTrain,res,fit,num_back,h.long)
  holder[i,] = xtNewFor
}

# Calculate percentiles for each column
percentiles_per_column <- apply(holder, 2, function(column) {
  quantile(column, probs = c(0.025, 0.975))
})

# Transpose the result for better readability
percentiles_per_column <- t(percentiles_per_column)

# Print the result
print(percentiles_per_column)

xtNewForBS = colMeans(holder)

# plotts.wge(c(xtTrain-est$res,xtNewForBS))
# plotts.wge(c(fit$fitted,xtNewForBS),ylim=c(min(fit$fitted)-.1,max(fit$fitted)+.1)) # black is predictions
plotts.wge(c(fit$fitted,xtNewForBS),ylim=c(12.4,max(fit$fitted)+.1),xlim=c(150,length(c(fit$fitted,xtNewForBS))), 
           main = 'Multivariate MLP 5 Year Forecast with Confidence Intervals') # black is predictions
lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,1],col = "blue") # blue are confidence intervals
lines(seq(lf+1,lf+h.long,1),percentiles_per_column[,2],col = "blue")
lines(seq(lf+1,lf+h.long,1),xt[(l-h.long+1):l],col = "red") # Actual values

```

## Evaluate the Model

Wording: 

In 1 year, we are 95% confident that the median home sale price will be between $X (e^X) and $X (e^X). Our best estimate is $415,169 (e^12.93644).

In 5 years, we are 95% confident that the median home sale price will be between $X (e^X) and $X (e^X). Our best estimate is $547,463 (e^13.21305).

```{r}

# Look at White Noise
resid.mlp.4 = log.mhp[195:198] - forecasted.mlp.X.d1.4$mean
plot(resid.mlp.4)
acf(resid.mlp.4)
hist(resid.mlp.4)
                            
resid.mlp.20 = log.mhp[179:198] - forecasted.mlp.X.d2.20$mean
plot(resid.mlp.20)
acf(resid.mlp.20)
hist(resid.mlp.20)
plotts.sample.wge(resid.mlp.20, arlimits = TRUE)

# Values for 1 yr and 5 yr Forecast
f.xreg.d1.4$mean[4]
f.xreg.d2.20$mean[20]

```

## LSTM

LSTMs are a type of Recurrent Neural Network (RNN) that have shown success with Time Series data.

The LSTM was created in Python.  The code is below.

```{python python code, eval=FALSE}
import torch
import torch.nn as nn
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import random

class TimeSeriesDataset(Dataset):
    def __init__(self, data, sequence_length):
        self.data = torch.FloatTensor(data)
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx):
        return (
            self.data[idx:idx+self.sequence_length],
            self.data[idx+self.sequence_length]
        )

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        # Initialize cell state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])
        return out

def train_model(model, train_loader, criterion, optimizer, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.unsqueeze(-1) # Ensure batch_x is 3-D: (batch_size, sequence_length, input_size)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y.unsqueeze(1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')

# After evaluation
def save_results_to_csv(predictions, actuals, ase, horizon_type="short"):
    results_df = pd.DataFrame({
        'Prediction': predictions,
        'Actual': actuals,
        'Error': [pred - act for pred, act in zip(predictions, actuals)]
    })
    
    # Add summary statistics
    summary_df = pd.DataFrame({
        'Metric': ['ASE'],
        'Value': [ase]
    })
    
    # Save to CSV files
    results_df.to_csv(f'predictions_{horizon_type}_term.csv', index=True)
    summary_df.to_csv(f'metrics_{horizon_type}_term.csv', index=False)

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Load and preprocess your data
df = pd.read_csv('https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSPUS.csv')
# import pdb
#pdb.set_trace()
data = df['MSPUS'].values[49:246]  # Select rows 49 through 245 inclusive

# After loading data but before normalization
data = np.log(data)  # Apply log transformation

# After loading data but before creating datasets
# Initialize the scaler
scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data.reshape(-1, 1)).flatten()

# Define parameters
# sequence_length = 10
sequence_length = 8
# sequence_length = 5
hidden_size = 64
# hidden_size = 128
num_layers = 2
# num_layers = 3
batch_size = 32
# batch_size = 16
num_epochs = 100
learning_rate = 0.01
# learning_rate = 0.005
h_short = 4
h_long = 20

# Create dataset and dataloader
# dataset = TimeSeriesDataset(data, sequence_length)
# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Split data into train and test
train_data = data_normalized[:-h_short]  # All data except last h_short entries
test_data = data_normalized[-sequence_length-h_short:]  # Last sequence_length + h_short entries

# Create datasets and dataloaders
train_dataset = TimeSeriesDataset(train_data, sequence_length)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = TimeSeriesDataset(test_data, sequence_length)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Initialize model, criterion, and optimizer
model = LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers)
# model = LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, dropout=0.2)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
train_model(model, train_loader, criterion, optimizer, num_epochs)

# Add evaluation mode
def evaluate_model(model, test_loader):
    model.eval()
    predictions = []
    actuals = []
    total_se = 0  # Initialize before the loop
    n = 0  # Initialize before the loop
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.unsqueeze(-1)  # Add this line to ensure 3D input
            output = model(batch_x)
            # Denormalize predictions and actuals
            pred = scaler.inverse_transform(output.numpy())[0][0]
            actual = scaler.inverse_transform(batch_y.numpy().reshape(-1, 1))[0][0]
            predictions.append(pred)
            actuals.append(actual)
            # predictions.append(output.item())
            # actuals.append(batch_y.item())

            # Then un-log transform
            pred = np.exp(pred)
            actual = np.exp(actual)

            # Calculate squared error for this prediction
            se = (pred - actual) ** 2
            total_se += se
            n += 1
            
    ase = total_se / 1e6 / n if n > 0 else 0
    return predictions, actuals, ase
    # return predictions, actuals

# Evaluate on test data
# predictions, actuals = evaluate_model(model, test_loader)
predictions, actuals, ase = evaluate_model(model, test_loader)
print("\nTest Results for short term prediction:")
for i, (pred, actual) in enumerate(zip(predictions, actuals)):
    print(f"Prediction {i+1}: {pred:.2f}, Actual: {actual:.2f}")
print(f"\nAverage Squared Error: {ase:,.2f}")

# Save results
save_results_to_csv(predictions, actuals, ase, "short")

# Also do it with the long term evaluation
train_data = data_normalized[:-h_long]  # All data except last h_long entries
test_data = data_normalized[-sequence_length-h_long:]  # Last sequence_length + h_long entries
train_dataset = TimeSeriesDataset(train_data, sequence_length)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = TimeSeriesDataset(test_data, sequence_length)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
train_model(model, train_loader, criterion, optimizer, num_epochs)
predictions, actuals, ase = evaluate_model(model, test_loader)
print("\nTest Results for long term prediction:")
for i, (pred, actual) in enumerate(zip(predictions, actuals)):
    print(f"Prediction {i+1}: {pred:.2f}, Actual: {actual:.2f}")
print(f"\nAverage Squared Error: {ase:,.2f}")
save_results_to_csv(predictions, actuals, ase, "long")
```

The short term prediction was the following:

```{r LSTM short term predictions}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_predictions_short_term.csv"
pred <- read.csv(file_path, header = TRUE)
pred
x = fed_housing_data$Median_Sales_Price
l = length(x)
plot(seq(1,l,1),x,type="b")
points(seq(l-h.short+1,l,1),pred$Prediction,type="b",pch=15,col="blue")
```

The short term ASE was 81.8 M.  This was slightly better than the ARIMA model.  Note that it is like our other ASEs, where it compares to the unlogged, original Medium Sales Prices.

```{r LSTM short term ASE}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_metrics_short_term.csv"
ase <- read.csv(file_path, header = TRUE)
ase
```

The long term prediction was the following:

```{r LSTM long term predictions}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_predictions_long_term.csv"
pred <- read.csv(file_path, header = TRUE)
pred
plot(seq(1,l,1),x,type="b")
points(seq(l-h.long+1,l,1),pred$Prediction,type="b",pch=15,col="blue")
```

The long term ASE was 149 M.  This was an order of magnitude better than the Signal Plus Noise model, which had the best long term ASE so far.  And the plot of the predictions (of the logged data) vs the actual values seems to line up almost exactly.

```{r LSTM long term ASE}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_metrics_long_term.csv"
ase <- read.csv(file_path, header = TRUE)
ase
```

```{r LSTM PPT Plots}
x = fed_housing_data$Median_Sales_Price
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_predictions_short_term.csv"
pred <- read.csv(file_path, header = TRUE)
plot(seq(xmin_plot,l,1),x[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="LSTM Short Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.short+1),l,1),pred$Prediction,col="red")
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/LSTM_predictions_long_term.csv"
pred <- read.csv(file_path, header = TRUE)
plot(seq(xmin_plot,l,1),x[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="LSTM Long Term Forecast",ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.long+1),l,1),pred$Prediction,col="red")
```

## TFT

TFT (Temporal Fusion Transformer) is an algorithm that uses Transformers and Attention to forecast Time Series data.  The model is known for being able to do very well with non-linear data, when it has tens of thousands or even hundreds of thousands of training examples.  We have less than 1000, so it is unlikely to do well on this dataset.

The TFT was created in Python.  The code is below.

```{python TFT python code, eval=FALSE}
# https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tft_model.html#darts.models.forecasting.tft_model.TFTModel

# from darts.datasets import WeatherDataset
from darts.models import TFTModel
from darts import TimeSeries
from darts.explainability.tft_explainer import TFTExplainer
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


# series = WeatherDataset().load()
series = pd.read_csv('https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/fed_housing_data.csv')

# Convert 'Year_Quarter' to datetime
# import pdb
# pdb.set_trace()
# series['Year_Quarter'] = pd.to_datetime(series['Year_Quarter'], format='%Y-Q%q')  # Adjust format as needed
series['Year_Quarter'] = pd.to_datetime(series['Year_Quarter'])

# Convert pandas Series/DataFrames to Darts TimeSeries
# Assuming 'Year_Quarter' is your time index
# First set the index to Year_Quarter
series = series.set_index('Year_Quarter')

# predicting atmospheric pressure
# target = series['p (mbar)'][:100]
h_short = 4
h_long = 20
# import pdb
# pdb.set_trace()
target = TimeSeries.from_dataframe(series[['Median_Sales_Price']][:-h_short])  # All data except last h_short entries

# optionally, past observed rainfall (pretending to be unknown beyond index 100)
# import pdb
# pdb.set_trace()
past_cov = TimeSeries.from_dataframe(series[['Ownership_Rate','Housing_Units_Completed','Supply_New_Houses','Housing_Price_Index']][:-h_short])

# future temperatures (pretending this component is a forecast)
# future_cov = series['T (degC)'][:106]

# by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model
# input_chunk_length = 12
input_chunk_length = 8
# n_epochs = 5
# n_epochs = 200
n_epochs = 20
random_state = 42
model = TFTModel(
    # input_chunk_length=8,
    input_chunk_length=input_chunk_length,
    output_chunk_length=h_short,
    hidden_size=128,              # Larger hidden size for more capacity
    lstm_layers=2,                # More LSTM layers
    num_attention_heads=4,        # More attention heads
    dropout=0.1,                  # Some dropout for regularization
    batch_size=16,                # Smaller batch size since we have limited data
    n_epochs=n_epochs,
    # learning_rate=0.001,          # Standard learning rate, ValueError: Invalid model creation parameters. Model `TFTModel` has no args/kwargs `['learning_rate']`
    random_state=random_state
)

# future_covariates are mandatory for `TFTModel`
# import pdb
# pdb.set_trace()
# future_cov = TimeSeries.from_dataframe(series[['Year_Quarter']])
# future_cov = TimeSeries.from_dataframe(series.iloc[:, 0])
future_cov = TimeSeries.from_dataframe(series.iloc[:, [0]])
model.fit(target, past_covariates=past_cov, future_covariates=future_cov)

# TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts
# pred = model.predict(h_short, num_samples=1000)

# Get mean prediction
# mean_pred = pred.mean()

# Get quantiles if you want confidence intervals
# quantiles_pred = pred.quantile([0.025, 0.975])  # 95% confidence interval
# import pdb
# pdb.set_trace()
# quantiles_pred = pred.quantile([0.025, 0.975])  # 95% confidence interval
# quantiles_pred_1 = pred.quantile(0.025)
# quantiles_pred_2 = pred.quantile(0.975)

def evaluate_model(model, h):
    """
    Evaluate the TFT model using multiple metrics
    """
    # Make predictions
    # predictions = model.predict(n=h_short, num_samples=100)
    predictions = model.predict(n=h, num_samples=1000)
    mean_predictions = predictions.mean()

    # Get confidence intervals
    # confidence_intervals = predictions.quantiles([0.025, 0.975])
    lower_bound = predictions.quantile(0.025)  # 2.5th percentile
    upper_bound = predictions.quantile(0.975)  # 97.5th percentile
    
    # Get actual values (last h_short values)
    # actuals = target[-h_short:]
    actuals = TimeSeries.from_dataframe(series[['Median_Sales_Price']][-h:])
    
    # Convert to numpy arrays for calculations
    pred_values = mean_predictions.values()
    actual_values = actuals.values()
    lower_values = lower_bound.values()
    upper_values = upper_bound.values()
    
    # Calculate metrics
    total_se = 0
    n = len(pred_values)
    predictions_list = []
    actuals_list = []
    lower_bounds_list = []
    upper_bounds_list = []
    
    for i in range(n):
        pred = pred_values[i][0]  # Assuming single-variable predictions
        actual = actual_values[i][0]
        lower = lower_values[i][0]
        upper = upper_values[i][0]
        
        # Store predictions and actuals
        predictions_list.append(pred)
        actuals_list.append(actual)
        lower_bounds_list.append(lower)
        upper_bounds_list.append(upper)

        # Then un-log transform
        pred = np.exp(pred)
        actual = np.exp(actual)
        
        # Calculate squared error
        se = (pred - actual) ** 2
        total_se += se
    
    # Calculate average squared error
    ase = total_se / 1e6 /n if n > 0 else 0
    
    return predictions_list, actuals_list, lower_bounds_list, upper_bounds_list, ase

# Use the evaluation function
preds, acts, lower_bounds, upper_bounds, ase = evaluate_model(model, h_short)

# Print results
print(f"Average Squared Error for short term prediction: {ase:,.2f}")
print("\nPredictions vs Actuals with 95% Confidence Intervals:")
for i, (p, a, l, u) in enumerate(zip(preds, acts, lower_bounds, upper_bounds)):
    print(f"Period {i+1}:")
    print(f"  Predicted = {p:,.2f} [{l:,.2f}, {u:,.2f}]")
    print(f"  Actual    = {a:,.2f}")
    print(f"  Diff      = {abs(p-a):,.2f}")
    print(f"  Width CI  = {u-l:,.2f}")

# Optional: Calculate additional metrics
# mape_score = mape(target[-h_short:], model.predict(n=h_short).mean())
# print(f"\nMAPE: {mape_score:.2%}")

# After training the model
# static_covariates = None  # If you have any static covariates, include them here

def save_detailed_results(preds, acts, lower_bounds, upper_bounds, ase, model_name="short_term"):
    # Predictions DataFrame
    results_df = pd.DataFrame({
        'Predicted': preds,
        'Actual': acts,
        'Lower_Bound': lower_bounds,
        'Upper_Bound': upper_bounds,
        'Absolute_Error': np.abs(np.array(preds) - np.array(acts)),
        'Squared_Error': (np.array(preds) - np.array(acts))**2,
        'CI_Width': np.array(upper_bounds) - np.array(lower_bounds),
        'Within_CI': (np.array(acts) >= np.array(lower_bounds)) & 
                    (np.array(acts) <= np.array(upper_bounds))
    })
    
    # Calculate summary metrics
    metrics_df = pd.DataFrame({
        'Metric': ['ASE', 'MAE', 'RMSE', 'Mean_CI_Width', 'Coverage_Rate'],
        'Value': [
            ase,
            np.mean(np.abs(np.array(preds) - np.array(acts))),
            np.sqrt(np.mean((np.array(preds) - np.array(acts))**2)),
            np.mean(np.array(upper_bounds) - np.array(lower_bounds)),
            np.mean((np.array(acts) >= np.array(lower_bounds)) & 
                   (np.array(acts) <= np.array(upper_bounds)))
        ]
    })
    
    # Save both files
    results_df.to_csv(f'predictions_detailed_{model_name}.csv', index=True)
    metrics_df.to_csv(f'metrics_detailed_{model_name}.csv', index=False)
    
    print(f"Detailed results saved to predictions_detailed_{model_name}.csv and metrics_detailed_{model_name}.csv")

save_detailed_results(preds, acts, lower_bounds, upper_bounds, ase, model_name="TFT_short_term")

# Get interpretability components
# interpretability = model.interpret_output(
# import pdb
# pdb.set_trace()
# interpretability = model.interpret(target,past_covariates=past_cov,future_covariates=future_cov,static_covariates=static_covariates)
explainer = TFTExplainer(model)
results = explainer.explain()

# Save the variable selection plot
# plt.figure(figsize=(12, 8))
explainer.plot_variable_selection(results)
# plt.savefig('variable_importance_short.png')  # Save to file
# plt.close()  # Close the plot to free memory

# Try with long term predictions as well
# import pdb
# pdb.set_trace()
target = TimeSeries.from_dataframe(series[['Median_Sales_Price']][:-h_long])
past_cov = TimeSeries.from_dataframe(series[['Ownership_Rate','Housing_Units_Completed','Supply_New_Houses','Housing_Price_Index']][:-h_long])
model = TFTModel(
    # input_chunk_length=8,
    input_chunk_length=input_chunk_length,
    output_chunk_length=h_long,
    hidden_size=128,              # Larger hidden size for more capacity
    lstm_layers=2,                # More LSTM layers
    num_attention_heads=4,        # More attention heads
    dropout=0.1,                  # Some dropout for regularization
    batch_size=16,                # Smaller batch size since we have limited data
    n_epochs=n_epochs,
    # learning_rate=0.001,        # Standard learning rate, ValueError: Invalid model creation parameters. Model `TFTModel` has no args/kwargs `['learning_rate']`
    random_state=random_state
)
model.fit(target, past_covariates=past_cov, future_covariates=future_cov)
preds, acts, lower_bounds, upper_bounds, ase = evaluate_model(model, h_long)
print(f"Average Squared Error for long term prediction: {ase:,.2f}")
print("\nPredictions vs Actuals with 95% Confidence Intervals:")
for i, (p, a, l, u) in enumerate(zip(preds, acts, lower_bounds, upper_bounds)):
    print(f"Period {i+1}:")
    print(f"  Predicted = {p:,.2f} [{l:,.2f}, {u:,.2f}]")
    print(f"  Actual    = {a:,.2f}")
    print(f"  Diff      = {abs(p-a):,.2f}")
    print(f"  Width CI  = {u-l:,.2f}")
save_detailed_results(preds, acts, lower_bounds, upper_bounds, ase, model_name="TFT_long_term")
explainer = TFTExplainer(model)
results = explainer.explain()
explainer.plot_variable_selection(results)
```

The short term prediction was the following:

```{r TFT short term predictions}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/predictions_detailed_TFT_short_term.csv"
pred <- read.csv(file_path, header = TRUE)
pred
h.short = 4
x = fed_housing_data$Median_Sales_Price
l = length(x)
plot(seq(1,l,1),x,type="b")
points(seq(l-h.short+1,l,1),pred$Predicted,type="b",pch=15,col="blue")
lines(seq(l-h.short+1,l,1),pred$Lower_Bound,lty=2, col="red", lwd=2)
lines(seq(l-h.short+1,l,1),pred$Upper_Bound,lty=2, col="red", lwd=2)
```

The short term ASE was 11.5 B.  This was significantly worse than other models.  The poor results aren't unexpected though, as there were so few data points to train on.

```{r TFT short term ASE}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/metrics_detailed_TFT_short_term.csv"
metrics <- read.csv(file_path, header = TRUE)
metrics # 1146
```

The long term prediction was the following:

```{r TFT long term predictions}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/predictions_detailed_TFT_long_term.csv"
pred <- read.csv(file_path, header = TRUE)
pred
h.long=20
x = fed_housing_data$Median_Sales_Price
l = length(x)
plot(seq(1,l,1),x,type="b")
points(seq(l-h.long+1,l,1),pred$Predicted,type="b",pch=15,col="blue")
lines(seq(l-h.long+1,l,1),pred$Lower_Bound,lty=2, col="red", lwd=2)
lines(seq(l-h.long+1,l,1),pred$Upper_Bound,lty=2, col="red", lwd=2)
```

The long term ASE was 15.0 B.  Similar to the short term predictions, this model did very poorly compared to the other models.

```{r TFT long term ASE}
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/metrics_detailed_TFT_long_term.csv"
metrics <- read.csv(file_path, header = TRUE)
metrics # 1146
```
# Ensemble

## Short Term Ensemble: VAR + MLP

Since the VAR model overpredicted and the MLP underpredicted for the short term forecast, we tried an ensemble model that averaged the two. This gave us an ASE of 34.9M, which is our lowest ASE so far. The RMSE was also our best metric too, at 0.0012, though that was based only on the last window and was not a rolling window. Ultimately, we decided to use a single model for the ensemble method instead of splitting it up for the sake of the presentation time.

```{r Ensemble Model Short Term}
log.mhp = fed_housing_data$Median_Sales_Price

# Refit VAR Model
x = fed_housing_data[1:194,]
l = length(x)
x$Year_Quarter = c()
VARselect(x,lag.max=16,type="both",season=NULL,exogen=NULL) # lag = 4
fit = VAR(x,p=4,type='both') 
summary(fit) # trend and const were significant, but only lag up to 2 for variable of interest, huc not very predictive
preds=predict(fit,n.ahead=h.short)

# VAR Metrics
ase = mean((fed_housing_data_NL$Median_Sales_Price[(l-h.short+1):l]-exp(preds$fcst$Median_Sales_Price[,1]))^2)
ase # 46.3595

# VAR Plot
plot(seq(1,198,1),fed_housing_data$Median_Sales_Price,type="b")
points(seq(195,198,1),preds$fcst$Median_Sales_Price[1:h.short,1],type="b",pch=15,col="blue")
fanchart(preds)

# Zoomed in VAR Plot
# VAR Plot
plot(seq(150,198,1),fed_housing_data$Median_Sales_Price[150:198],type="b")
points(seq(195,198,1),preds$fcst$Median_Sales_Price[1:h.short,1],type="b",pch=15,col="blue")
fanchart(preds)

# Refit Univariate MLP Model
log.mhp = fed_housing_data$Median_Sales_Price
msp.194 = ts(log.mhp[1:194])

# 1 Year Horizon
mspFit.4 = mlp(msp.194, comb = 'median')
f.4 = forecast(mspFit.4, h = 4)

# MLP Metrics
ASE.mlp.exp.h4 = mean((exp(log.mhp[195:198]) - exp(f.4$mean))^2) # 60.95M
rwfit.mlp4 = roll.win.rmse.nn.wge(log.mhp, horizon = 4, fit_model = mspFit.4) # 0.029

# Forecasts
plot(log.mhp, type = 'l', lwd = 1, main = 'Univariate Forecasts, 1 and 5 Years')
points(seq(195,198), f.4$mean, type = 'l', col = 'red')

# Zoomed In, 1 yr
plot(seq(150,198,1),log.mhp[150:198], type = 'l', lwd = 1, main = 'Univariate MLP Forecast, 1 Year')
points(seq(195,198,1), f.4$mean, type = 'l', col = 'red')

# Ensemble Model
ensemble.4 = (preds$fcst$Median_Sales_Price[,1] + f.4$mean)/2
ensemble.4

# Forecast
plot(log.mhp, type = 'l', ylim=c(10.5, 13.1),
     main = 'Ensemble Model, VAR & MLP Short Term Forecast', xlab = 'Time', 
     ylab = 'log Median Housing Sales Price')
lines(seq(195,198,1),ensemble.4,col = "red")

# Zoomed In
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim=c(12.4, 13.1),
     main = 'Ensemble Model, VAR & MLP Short Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price')
lines(seq(195,198,1),ensemble.4,col = "red")

# Zoomed In with original models
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim=c(12.4, 13.1),
     main = 'Ensemble Model, VAR & MLP Short Term Forecast', 
     sub = 'The ensemble model (red) takes the average of the VAR (blue) and MLP (green) models',
     xlab = 'Time', ylab = 'log Median Housing Sales Price')
lines(seq(195,198,1),ensemble.4,col = "red")
points(seq(195,198,1),preds$fcst$Median_Sales_Price[1:h.short,1],type="l", col="blue")
points(seq(195,198,1), f.4$mean, type = 'l', col = 'green')

# Metrics
ASE.ensemble.4 = mean((exp(log.mhp[195:198]) - exp(ensemble.4))^2) # 63.36M
ASE.ensemble.4
RMSE.ensemble.4 = sqrt(mean(log.mhp[195:198] - ensemble.4)^2) # 0.0089
RMSE.ensemble.4
```

## Long Term Ensemble: SPN + MLP

We tested an ensemble method for SPN and MLP just to see how it looked. There was not a compelling reason since they both had the same behavior.
```{r Ensemble Model Long Term}
log.mhp = fed_housing_data$Median_Sales_Price

# SPN Fit with MLE estimates, using all data
fit.mle.sig_h20 = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, 
                                        max.p = 6, n.ahead = 20, lastn = TRUE)

# SPN Plot
plot(log.mhp, type = 'l')
lines(seq(179,198,1), fit.mle.sig_h20$f, col = "red")
lines(seq(179,198,1), fit.mle.sig_h20$ll, lty = 3, col = "blue")
lines(seq(179,198,1), fit.mle.sig_h20$ul, lty = 3, col = "blue")

# Zoomed In
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim = c(12.3, 13.2), main = "Linear Signal with AR(6) Noise Long Term Forecast", 
     xlab = "Time", ylab = "log Median Housing Sales Price")
lines(seq(179,198,1), fit.mle.sig_h20$f, col = "red")
lines(seq(179,198,1), fit.mle.sig_h20$ll, lty = 3, col = "blue")
lines(seq(179,198,1), fit.mle.sig_h20$ul, lty = 3, col = "blue")

# ASE with MLE estimates
ASE.h20 = mean((log.mhp[179:198] - fit.mle.sig_h20$f)^2)
ASEexp.h20 = mean((exp(log.mhp[179:198]) - exp(fit.mle.sig_h20$f))^2)
ASEexp.h20 # 1.1B

# MLP Model
msp.178 = ts(log.mhp[1:178])

# 5 Year Horizon
mspFit.20 = mlp(msp.178, comb = 'median')
f.20 = forecast(mspFit.20, h = 20)

# Metrics
ASE.mlp.exp.h20 = mean((exp(log.mhp[179:198]) - exp(f.20$mean))^2) # 1.72B
rwfit.mlp20 = roll.win.rmse.nn.wge(log.mhp, horizon = 20, fit_model = mspFit.20) # 0.072

# Forecasts
plot(log.mhp, type = 'l', lwd = 1, main = 'MLP Univariate Long Term Forecast',
     ylab = 'log Median Housing Sales Price', xlab = 'Time')
points(seq(179,198), f.20$mean, type = 'l', col = 'red')

# Zoomed In, 5 yr
plot(seq(150,198,1),log.mhp[150:198], type = 'l', main = 'Univariate MLP Forecast, 1 Year',
     ylab = 'log Median Housing Sales Price', xlab = 'Time')
points(seq(179,198,1), f.20$mean, type = 'l', col = 'red')

# Ensemble Model
ensemble.20 = (fit.mle.sig_h20$f + f.20$mean)/2
ensemble.20

# Forecast
plot(log.mhp, type = 'l', ylim=c(10.5, 13.1),
     main = 'Ensemble Model, SPN & MLP Long Term Forecast', xlab = 'Time', 
     ylab = 'log Median Housing Sales Price')
lines(seq(179,198,1),ensemble.20,col = "red")

# Zoomed In
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim=c(12.4, 13.1),
     main = 'Ensemble Model, SPN & MLP Long Term Forecast', 
     xlab = 'Time', ylab = 'log Median Housing Sales Price')
lines(seq(179,198,1),ensemble.20,col = "red")

# Zoomed in with Original Models
plot(seq(150,198,1),log.mhp[150:198], type = 'l', ylim=c(12.4, 13.1),
     main = 'Ensemble Model, SPN & MLP Long Term Forecast', 
     sub = 'The ensemble model (red) takes the average of the SPN (orange) and the MLP (green) forecasts',  
     xlab = 'Time', ylab = 'log Median Housing Sales Price')
lines(seq(179,198,1),ensemble.20,col = "red")
points(seq(179,198,1), f.20$mean, type = 'l', col = 'green')
lines(seq(179,198,1), fit.mle.sig_h20$f, col = "orange")

# Metrics
ASE.ensemble.20 = mean((exp(log.mhp[179:198]) - exp(ensemble.20))^2) # 1.658B
ASE.ensemble.20
RMSE.ensemble.20 = sqrt(mean(log.mhp[179:198] - ensemble.20)^2) # 0.0758
RMSE.ensemble.20
```

## Ensemble: MLR + SPN

The long term plots for MLR seemed to be too low, and the long term plots for SPN seemed to be too high.  So we thought that combining them could improve the long term predictions.

```{r Ensemble Model MLR + SPN}
# MLR forecasts
x = fed_housing_data
x.short = fed_housing_data_short
x.long = fed_housing_data_long
x$Year_Quarter = c()
x.short$Year_Quarter = c()
x.long$Year_Quarter = c()
l = length(x$Median_Sales_Price)
t=1:l
t.train.short= 1:(l-h.short)
t.test.short=(l-h.short+1):l
t.train.long= 1:(l-h.long)
t.test.long=(l-h.long+1):l
x$Housing_Units_Completed_l21 = dplyr::lag(x$Housing_Units_Completed,21)
x$Supply_New_Houses_l9 = dplyr::lag(x$Supply_New_Houses,9)
x.short$Housing_Units_Completed_l21 = dplyr::lag(x.short$Housing_Units_Completed,21)
x.short$Supply_New_Houses_l9 = dplyr::lag(x.short$Supply_New_Houses,9)
x.long$Housing_Units_Completed_l21 = dplyr::lag(x.long$Housing_Units_Completed,21)
x.long$Supply_New_Houses_l9 = dplyr::lag(x.long$Supply_New_Houses,9)
fit=arima(x.short$Median_Sales_Price[t.train.short],order=c(2,0,0),xreg=cbind(t.train.short,x.short$Housing_Units_Completed_l21[t.train.short],x.short$Supply_New_Houses_l9[t.train.short],x.short$Housing_Price_Index[t.train.short]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.short$Housing_Units_Completed_l21[t.test.short],Supply_New_Houses_l9=x.short$Supply_New_Houses_l9[t.test.short],Housing_Price_Index=x.short$Housing_Price_Index[t.test.short]))
mlr_pred_short = preds$pred
fit=arima(x.long$Median_Sales_Price[t.train.long],order=c(2,0,0),xreg=cbind(t.train.long,x$Housing_Units_Completed_l21[t.train.long],x$Supply_New_Houses_l9[t.train.long],x$Housing_Price_Index[t.train.long]))
preds = predict(fit,newxreg = data.frame(t=t.test.short,Housing_Units_Completed_l21=x.long$Housing_Units_Completed_l21[t.test.long],Supply_New_Houses_l9=x.long$Supply_New_Houses_l9[t.test.long],Housing_Price_Index=x.long$Housing_Price_Index[t.test.long]))
mlr_pred_long = preds$pred

# SPN forecasts
log.mhp = fed_housing_data$Median_Sales_Price
fit.mle.sig_h4 = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 4, lastn = TRUE,plot=FALSE)
spn_pred_short = fit.mle.sig_h4$f
fit.mle.sig_h20 = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 20, lastn = TRUE,plot=FALSE)
spn_pred_long = fit.mle.sig_h20$f

# Creating ensemble forecasts
ensemble_mlr_spn_short = (mlr_pred_short + spn_pred_short)/2
ensemble_mlr_spn_long = (mlr_pred_long + spn_pred_long)/2

# Metrics
ase = mean((fed_housing_data_NL$Median_Sales_Price[(l-h.short+1):l]-exp(ensemble_mlr_spn_short))^2)/1e6
ase # 207.8022
ase = mean((fed_housing_data_NL$Median_Sales_Price[(l-h.long+1):l]-exp(ensemble_mlr_spn_long))^2)/1e6
ase # 653.9763

# Plots
plot(seq(xmin_plot,l,1),x$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price", main="Ensemble Model, MLR & SPN Short Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts', ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.short+1),l,1),ensemble_mlr_spn_short,col="red")
lines(seq((l-h.short+1),l,1),mlr_pred_short,col="orange")
lines(seq((l-h.short+1),l,1),spn_pred_short,col="green")
plot(seq(xmin_plot,l,1),x$Median_Sales_Price[xmin_plot:l],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price", main="Ensemble Model, MLR & SPN Long Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts' ,ylim=c(ymin_plot,ymax_plot))
lines(seq((l-h.long+1),l,1),ensemble_mlr_spn_long,col="red")
lines(seq((l-h.long+1),l,1),mlr_pred_long,col="orange")
lines(seq((l-h.long+1),l,1),spn_pred_long,col="green")
```

This model has the best long term ASE, and the plots for the short term forecast seem reasonable.  Now we will forecast out to 1 and 5 years with this ensemble.

```{r Ensemble Model MLR + SPN future forecasts}
# SPN 
fit.mle.sig_h4_ahead = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 4, lastn = FALSE, plot = FALSE)
fit.mle.sig_h20_ahead = fore.sigplusnoise.wge(log.mhp, linear = TRUE, method = 'mle', freq = 0, max.p = 6, n.ahead = 20, lastn = FALSE, plot=FALSE)

# MLR
x = fed_housing_data_forecast
x$Housing_Units_Completed_l21 = dplyr::lag(x$Housing_Units_Completed,21)
x$Supply_New_Houses_l9 = dplyr::lag(x$Supply_New_Houses,9)
t=1:l
t.fore.short = (l+1):(l+h.short)
t.fore.long = (l+1):(l+h.long)
fit=arima(x$Median_Sales_Price[t],order=c(2,0,0),xreg=cbind(t,x$Housing_Units_Completed_l21[t],x$Supply_New_Houses_l9[t],x$Housing_Price_Index[t]))

# Ensemble
preds_short = predict(fit,newxreg = data.frame(t=t.fore.short,Housing_Units_Completed_l21=x$Housing_Units_Completed_l21[t.fore.short],Supply_New_Houses_l9=x$Supply_New_Houses_l9[t.fore.short],Housing_Price_Index=x$Housing_Price_Index[t.fore.short]))
ensemble_fore_short = (fit.mle.sig_h4_ahead$f + preds_short$pred)/2
preds_long = predict(fit,newxreg = data.frame(t=t.fore.long,Housing_Units_Completed_l21=x$Housing_Units_Completed_l21[t.fore.long],Supply_New_Houses_l9=x$Supply_New_Houses_l9[t.fore.long],Housing_Price_Index=x$Housing_Price_Index[t.fore.long]))
ensemble_fore_long = (fit.mle.sig_h20_ahead$f + preds_long$pred)/2
  
# Plots
plot(seq(xmin_plot,l+h.short,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.short)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="Ensemble Model, MLR & SPN Short Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts',ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.short),1),ensemble_fore_short,col="red")
lines(seq((l+1),(l+h.short),1),preds_short$pred,col="orange")
lines(seq((l+1),(l+h.short),1),fit.mle.sig_h4_ahead$f,col="green")
plot(seq(xmin_plot,l+h.long,1),fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.long)],type="l",col="black",xlab="Time",ylab="log Median Housing Sales Price",main="Ensemble Model, MLR & SPN Long Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts',ylim=c(ymin_plot,ymax_future))
lines(seq((l+1),(l+h.long),1),ensemble_fore_long,col="red")
lines(seq((l+1),(l+h.long),1),fit.mle.sig_h20_ahead$f,col="orange")
lines(seq((l+1),(l+h.long),1),preds_long$pred,col="green")

# Plots with actual data
plot(seq(xmin_plot,l+h.short,1),exp(fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.short)])/1e3,type="l",col="black",xlab="Time",ylab="Median Housing Sales Price (in thousands of dollars)",main="Ensemble Model, MLR & SPN Short Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts',ylim=c(250,650))
lines(seq((l+1),(l+h.short),1),exp(ensemble_fore_short)/1e3,col="red")
lines(seq((l+1),(l+h.short),1),exp(preds_short$pred)/1e3,col="orange")
lines(seq((l+1),(l+h.short),1),exp(fit.mle.sig_h4_ahead$f)/1e3,col="green")
plot(seq(xmin_plot,l+h.long,1),exp(fed_housing_data$Median_Sales_Price[xmin_plot:(l+h.long)])/1e3,type="l",col="black",xlab="Time",ylab="Median Housing Sales Price (in thousands of dollars)",main="Ensemble Model, MLR & SPN Long Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts',ylim=c(250,650))
lines(seq((l+1),(l+h.long),1),exp(ensemble_fore_long)/1e3,col="red")
lines(seq((l+1),(l+h.long),1),exp(fit.mle.sig_h20_ahead$f)/1e3,col="orange")
lines(seq((l+1),(l+h.long),1),exp(preds_long$pred)/1e3,col="green")

# Plots at full timescale
plot(seq(1,l+h.long,1),exp(fed_housing_data$Median_Sales_Price[1:(l+h.long)])/1e3,type="l",col="black",xlab="Time",ylab="Median Housing Sales Price (in thousands of dollars)",main="Ensemble Model, MLR & SPN Long Term Forecast", sub = 'The ensemble model (red) is the average of the MLR (orange) and the SPN (green) forecasts',ylim=c(0,650))
lines(seq((l+1),(l+h.long),1),exp(ensemble_fore_long)/1e3,col="red")
lines(seq((l+1),(l+h.long),1),exp(fit.mle.sig_h20_ahead$f)/1e3,col="orange")
lines(seq((l+1),(l+h.long),1),exp(preds_long$pred)/1e3,col="green")
```