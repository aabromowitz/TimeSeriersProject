---
title: "Time Series Project"
author: "Aaron Abromowitz and Alex Thibeaux"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r, message=FALSE, "Load Libraries"}
library(tidyverse)
library(tswge)
library(vars)
library(lubridate)
```

# Load Data
```{r Load and wrangle data}

# Variable of Interest - Quarterly from 1/1/63
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSPUS.csv"
mhp <- read.csv(file_path, header = TRUE)

# Home ownership rate - Quarterly from 1/1/65
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/RHORUSQ156N.csv"
hor <- read.csv(file_path, header = TRUE)

# Housing units completed - Monthly from 1/1/1968
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/COMPUTSA.csv"
huc <- read.csv(file_path, header = TRUE)

# Supply of new houses - Monthly from 1/1/1963
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSACSR.csv"
snh <- read.csv(file_path, header = TRUE)

# House price index - Quarterly from 1/1/1975
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/USSTHPI.csv"
hpi <- read.csv(file_path, header = TRUE)

# Converting Monthly Data to Quarterly Data

# Preserve Monthly format
snh_monthly <- snh
huc_monthly <- huc

# Supply of New Houses Variable 
snh$DATE = as.Date(snh$DATE)
snh$month <- month(snh$DATE)
head(snh)
snh_quarterly <- snh %>%
  filter(snh$month == 1 | snh$month == 4 | snh$month == 7 | snh$month == 10)
summary(snh_quarterly)

# Housing Units Completed Variable
huc$DATE = as.Date(huc$DATE)
huc$month <- month(huc$DATE)
head(huc)
huc_quarterly <- huc %>%
  filter(huc$month == 1 | huc$month == 4 | huc$month == 7 | huc$month == 10)
summary(huc_quarterly)

# Using same time frames, which would be starting at 1975 Q1 and ending at 2024 Q2 (due to hpi data)

# hor observation 41 is 1975 Q1
hor_1975 = hor[41:238,]
hor_1975$DATE <- as.Date(hor_1975$DATE)
summary(hor_1975)

# huc_quarterlly observation 29
huc_1975 = huc_quarterly[29:226,]
summary(huc_1975)

# mhp observation 49 is 1975 Q1
mhp_1975 = mhp[49:246,]
mhp_1975$DATE <- as.Date(mhp_1975$DATE)
summary(mhp_1975)

# snh_quarterly observation 49 is 1975 Q1
snh_1975 = snh_quarterly[49:246,]
summary(snh_1975)

# Housing Price Index variable already from 1975 Q1 - 2024 Q2
hpi$DATE <- as.Date(hpi$DATE)
summary(hpi)

# Create Dataframe
# Combined (Train/Test) Set NOT LOGGED
fed_housing_data_NL = data.frame(Year_Quarter = mhp_1975$DATE, Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = mhp_1975$MSPUS)

# Combined (Train/Test) Set LOGGED
fed_housing_data = data.frame(Year_Quarter = as.Date(mhp_1975$DATE), Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = log(mhp_1975$MSPUS))

# Train & Test sets
train = fed_housing_data[1:168,]
test = fed_housing_data[169:198,]

summary(fed_housing_data)
```

# EDA

## Median Housing Sale Price

Our variable of interest is the Median Housing Sale Price. This was quarterly data starting in 1965.

```{r Plot Median Housing Sales Price}
mhp=mhp$MSPUS
plot(ts(mhp/1000, frequency=4,start=c(1965,1)),xlab='Year',ylab='Housing price (in thousands of dollars)')
title(main='Median US Housing Price from 1965')
acf(mhp,ylab='',main='ACF')
x = parzen.wge(mhp)
```

The realization looks like it is increasing linearly. This data could be model with a signal plus noise, with a linear signal.

The realization appears to be non stationary. There is evidence that the data is increasing over time. In addition, the variation appears much higher in later years as in earlier years, showing that variance is increasing over time as well. When modeling, it may be useful to take the logarithm of the data before modeling.

The ACF shows very slowly dampening autocorrelations. The parzen window also shows a very low frequency. This could points to a (1-B) term which could be removed with a high-pass difference filter.

## Home Ownership Rate

We used 4 exogenous variables. The first was home ownership rate. Like the variable of interest, this was quarterly data starting from 1965.

```{r Plot Home Ownership Rate}
hor <- hor$RHORUSQ156N
plot(ts(hor, frequency=4,start=c(1965,1)),xlab='Year',ylab='Percentage')
title(main='Home Ownership Rate from 1965')
acf(hor,ylab='',main='ACF')
x = parzen.wge(hor)
```

Unlike the median house price, the home ownership rate appears to be stationary. There isn’t a linear trend to the data, and the variance seems to be constant over time.

The ACF does show that there are dampening autocorrelations, but not quite at the rate as for median house price. This could point to an ARMA model being appropriate for the home ownership rate.

Similar to median house price, the Parzen Window shows that 0 is a prominent frequency.

## Housing Units Completed

The next exogenous variable is Housing Units Completed. This was a monthly variable, starting from 1968.

```{r Plot Housing Units Complete}
huc <- huc$COMPUTSA
plot(ts(huc, frequency=12,start=c(1968,1)),xlab='Year',ylab='')
title(main='Housing Units Completed from 1968')
acf(huc,ylab='',main='ACF')
x = parzen.wge(huc)
```

This has very similar characteristics to home ownership rate. It’s autocorrelations dampen a little more slowly, but the ACF and Parzen Window look very similar. This variable likewise shows evidence for being stationary.

## Supply of New Houses

```{r Plot Supply of New Houses}
snh <- snh$MSACSR
plot(ts(snh, frequency=12,start=c(1963,1)),xlab='Year',ylab='')
title(main='Supply of New Houses from 1963')
acf(snh,ylab='',main='ACF')
x = parzen.wge(snh)
```

Similar to home ownership rate as well, but with even more quickly dampening autocorrelations. This variable likewise shows evidence for being stationary.

## House Price Index

The next exogenous variable is House Price Index. This was a quarterly variable, starting from 1975.

```{r Plot House Price Index}
hpi <- hpi$USSTHPI
plot(ts(hpi, frequency=4,start=c(1975,1)),xlab='Year',ylab='')
title(main='House Price Index from 1975')
acf(hpi,ylab='',main='ACF')
x = parzen.wge(hpi)
```

The realization has a similar upward trend to the Housing Sale Price. It seems very smooth though, very little variance. Because of the upward trend though, it shows evidence against stationarity.

The ACF and Parzen Window are also similar to those of Housing Sale Price.

Not all the data matches up by date range or by frequency of observation (monthly vs quarterly).  To make everything consistent, we will only use quarterly data going from Q1 1975 to Q2 2024.

# AR(I)MA Model Investigation

## First ARIMA Model

The first model we looked at was an ARIMA model.  From the EDA, we saw that because of the increasing variance, we should look at the log of the Housing Sales Price.  And since the Parzen Window had a prominent peak at 0 and very slowly dampening autocorrelations, we should take the first difference.

```{r Difference Median Housing Price}
log.mhp = fed_housing_data$Median_Sales_Price
mhp = exp(log.mhp)
d.log.mhp = artrans.wge(log.mhp,1)
```

The difference looks more like white noise without an obvious pattern to the ACF.  Next we will try modeling the differenced data as an ARMA model.

```{r ARIMA(p,1,q) p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 2.

We will evaluate our models with a short term horizon of 1 year (4 quarters) and a long term horizon of 5 years (20 quarters).  We calcluated a Rolling Window RMSE for both of those horizons (using the logged data) and calculated an ASE for both of those horizons as well (taking the exponential to get back to the original data).

```{r ARIMA(p,1,q) metrics}
h.short = 4
h.long = 20
l = length(mhp)
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 84.37902
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 7091.032
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=1,phi=est$phi,theta=est$theta) # 0.036501
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=1,phi=est$phi,theta=est$theta) # 0.13269
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,1,2) model we get a short term Rolling Window RMSE of 0.037, a long term Rolling Window RMSE of 0.133, a short term ASE of 84.4 million and a long term ASE of 7.09 billion.

## ARMA Model

As a comparison, we wanted to look a simpler ARMA model.  

```{r ARMA p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='bic')
```

Both the AIC and BIC selection choose p = 2 and q = 2.

```{r ARMA metrics}
est = est.arma.wge(log.mhp,p=2,q=2)
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 112.4314
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 9167.59
```
```{r ARMA Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,phi=est$phi,theta=est$theta) # 0.044488
```
```{r ARMA Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARMA Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,phi=est$phi,theta=est$theta) # 0.12779
```
```{r ARMA Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARMA(2,2) model we get a short term Rolling Window RMSE of 0.044, a long term Rolling Window RMSE of 0.128, a short term ASE of 112.4 million and a long term ASE of 9.17 billion.  Even though this model has a better long term Rolling Window RMSE than the ARIMA model, it did worse in the other metrics.

## ARIMA(p,2,q) model

Looking at the overfit factor table, we can see two prominent 0 frequency roots.  

```{r factor table}
est = est.ar.wge(log.mhp,p=12)
factor.wge(phi=est$phi)
```

Even though the differenced data didn't have slowly dampening autocorrelations and looked liked white noise, the original data had a pretty clear upward trend.  We thought that this warranted investigation into an ARIMA(p,2,q) model.

```{r ARIMA(p,2,q) p and q estimation}
d2.log.mhp = artrans.wge(d.log.mhp,1)
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 1.

```{r ARIMA(1,2,1) metrics}
est = est.arma.wge(d2.log.mhp,p=1,q=1)
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 162.1383
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 6263.556
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=2,phi=est$phi,theta=est$theta) # 0.049161
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=1,phi=est$phi,theta=est$theta) # 0.15465
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,2,1) model we get a short term Rolling Window RMSE of 0.049, a long term Rolling Window RMSE of 0.155, a short term ASE of 162.4 million and a long term ASE of 6.26 billion.  Even though this model had the best long term ASE so far, it did worse in the other metrics.

Comparing all three ARMA/ARIMA models, we will choose the ARIMA(1,1,2).  

```{r ARIMA(1,2,2) model characteristics}
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
aic = est$aic
aic # -7.188559
resid = f$resid
xbar=est$xbar
xbar # 0.01211603
vara = est$avar
vara # 0.0007251235
est$phi # 0.6901326
est$theta # 0.9720137 -0.4007360
```

The model (for the log data) can be written as:
(1-B)(1-0.69B)(x_t-0.012) = (1-0.972B+0.401B^2)a_t 
$\sigma$_t^2 = 0.001

Now we will look at residuals to make sure that they are white noise.

```{r ARIMA(1,2,2) residual plots}
plotts.wge(resid)
acf(resid,lag.max=100) 
parzen.wge(resid)
```

They appear to be white noise, and the ACF has very few values above 0.2.  We will run a Ljung-Box test to be sure.

```{r ARIMA(1,2,2) Ljung test}
ljung.wge(resid,K=24,p=1,q=2) # p = 0.2272667, white
ljung.wge(resid,K=48,p=1,q=2) # p = 0.5584003, white
```

The test gives more evidence that the residuals are white noise.  We can also check if they are normally distributed with a histogram.

```{r ARIMA(1,2,2) residual histogram}
hist(resid)
```

We also want to make sure that the ACFs and Parzen Windows generated from ARIMA(1,2,2) models look similar to that of our original data.

```{r ARIMA(1,2,2) generated ACFs and Parzen Windows}
# Multiple ACFs 
set.seed(2)
sims = 10
ACF = acf(log.mhp, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)
for( i in 1: sims)
{
  ACF2 = acf(gen.arima.wge(l, phi = est$phi, theta=est$theta,d=1, plot="FALSE"), plot = "FALSE")
  lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

# Multiple Parzen 
set.seed(3)
sims = 10
SpecDen = parzen.wge(log.mhp, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6)
for( i in 1: sims)
{
  SpecDen2 = parzen.wge(gen.aruma.wge(l,phi=est$phi, theta=est$theta,d=1, plot ="FALSE"), plot = "FALSE")
  lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}
```

The ACFs don't match up exactly, but have the same slowly dampening behavior.  The Parzen Windows are very close to the original.