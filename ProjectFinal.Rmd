---
title: "Time Series Project"
author: "Aaron Abromowitz and Alex Thibeaux"
date: "2024-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r, message=FALSE, "Load Libraries"}
library(tidyverse)
library(tswge)
library(vars)
library(lubridate)
library(vars)
```

# Load Data
```{r Load and wrangle data}

# Variable of Interest - Quarterly from 1/1/63
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSPUS.csv"
mhp <- read.csv(file_path, header = TRUE)

# Home ownership rate - Quarterly from 1/1/65
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/RHORUSQ156N.csv"
hor <- read.csv(file_path, header = TRUE)

# Housing units completed - Monthly from 1/1/1968
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/COMPUTSA.csv"
huc <- read.csv(file_path, header = TRUE)

# Supply of new houses - Monthly from 1/1/1963
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/MSACSR.csv"
snh <- read.csv(file_path, header = TRUE)

# House price index - Quarterly from 1/1/1975
file_path = "https://raw.githubusercontent.com/aabromowitz/TimeSeriersProject/refs/heads/main/USSTHPI.csv"
hpi <- read.csv(file_path, header = TRUE)

# Converting Monthly Data to Quarterly Data

# Preserve Monthly format
snh_monthly <- snh
huc_monthly <- huc

# Supply of New Houses Variable 
snh$DATE = as.Date(snh$DATE)
snh$month <- month(snh$DATE)
head(snh)
snh_quarterly <- snh %>%
  filter(snh$month == 1 | snh$month == 4 | snh$month == 7 | snh$month == 10)
summary(snh_quarterly)

# Housing Units Completed Variable
huc$DATE = as.Date(huc$DATE)
huc$month <- month(huc$DATE)
head(huc)
huc_quarterly <- huc %>%
  filter(huc$month == 1 | huc$month == 4 | huc$month == 7 | huc$month == 10)
summary(huc_quarterly)

# Using same time frames, which would be starting at 1975 Q1 and ending at 2024 Q2 (due to hpi data)

# hor observation 41 is 1975 Q1
hor_1975 = hor[41:238,]
hor_1975$DATE <- as.Date(hor_1975$DATE)
summary(hor_1975)

# huc_quarterlly observation 29
huc_1975 = huc_quarterly[29:226,]
summary(huc_1975)

# mhp observation 49 is 1975 Q1
mhp_1975 = mhp[49:246,]
mhp_1975$DATE <- as.Date(mhp_1975$DATE)
summary(mhp_1975)

# snh_quarterly observation 49 is 1975 Q1
snh_1975 = snh_quarterly[49:246,]
summary(snh_1975)

# Housing Price Index variable already from 1975 Q1 - 2024 Q2
hpi$DATE <- as.Date(hpi$DATE)
summary(hpi)

# Create Dataframe
# Combined (Train/Test) Set NOT LOGGED
fed_housing_data_NL = data.frame(Year_Quarter = mhp_1975$DATE, Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = mhp_1975$MSPUS)

# Combined (Train/Test) Set LOGGED
fed_housing_data = data.frame(Year_Quarter = as.Date(mhp_1975$DATE), Ownership_Rate = hor_1975$RHORUSQ156N, Housing_Units_Completed = huc_1975$COMPUTSA, Supply_New_Houses = snh_1975$MSACSR, Housing_Price_Index = hpi$USSTHPI, Median_Sales_Price = log(mhp_1975$MSPUS))

# Train & Test sets
train = fed_housing_data[1:168,]
test = fed_housing_data[169:198,]

summary(fed_housing_data)
```

# EDA

## Median Housing Sale Price

Our variable of interest is the Median Housing Sale Price. This was quarterly data starting in 1965.

```{r Plot Median Housing Sales Price}
mhp=mhp$MSPUS
plot(ts(mhp/1000, frequency=4,start=c(1965,1)),xlab='Year',ylab='Housing price (in thousands of dollars)')
title(main='Median US Housing Price from 1965')
acf(mhp,ylab='',main='ACF')
x = parzen.wge(mhp)
```

The realization looks like it is increasing linearly. This data could be model with a signal plus noise, with a linear signal.

The realization appears to be non stationary. There is evidence that the data is increasing over time. In addition, the variation appears much higher in later years as in earlier years, showing that variance is increasing over time as well. When modeling, it may be useful to take the logarithm of the data before modeling.

The ACF shows very slowly dampening autocorrelations. The parzen window also shows a very low frequency. This could points to a (1-B) term which could be removed with a high-pass difference filter.

## Home Ownership Rate

We used 4 exogenous variables. The first was home ownership rate. Like the variable of interest, this was quarterly data starting from 1965.

```{r Plot Home Ownership Rate}
hor <- hor$RHORUSQ156N
plot(ts(hor, frequency=4,start=c(1965,1)),xlab='Year',ylab='Percentage')
title(main='Home Ownership Rate from 1965')
acf(hor,ylab='',main='ACF')
x = parzen.wge(hor)
```

Unlike the median house price, the home ownership rate appears to be stationary. There isn’t a linear trend to the data, and the variance seems to be constant over time.

The ACF does show that there are dampening autocorrelations, but not quite at the rate as for median house price. This could point to an ARMA model being appropriate for the home ownership rate.

Similar to median house price, the Parzen Window shows that 0 is a prominent frequency.

## Housing Units Completed

The next exogenous variable is Housing Units Completed. This was a monthly variable, starting from 1968.

```{r Plot Housing Units Complete}
huc <- huc$COMPUTSA
plot(ts(huc, frequency=12,start=c(1968,1)),xlab='Year',ylab='')
title(main='Housing Units Completed from 1968')
acf(huc,ylab='',main='ACF')
x = parzen.wge(huc)
```

This has very similar characteristics to home ownership rate. It’s autocorrelations dampen a little more slowly, but the ACF and Parzen Window look very similar. This variable likewise shows evidence for being stationary.

## Supply of New Houses

```{r Plot Supply of New Houses}
snh <- snh$MSACSR
plot(ts(snh, frequency=12,start=c(1963,1)),xlab='Year',ylab='')
title(main='Supply of New Houses from 1963')
acf(snh,ylab='',main='ACF')
x = parzen.wge(snh)
```

Similar to home ownership rate as well, but with even more quickly dampening autocorrelations. This variable likewise shows evidence for being stationary.

## House Price Index

The next exogenous variable is House Price Index. This was a quarterly variable, starting from 1975.

```{r Plot House Price Index}
hpi <- hpi$USSTHPI
plot(ts(hpi, frequency=4,start=c(1975,1)),xlab='Year',ylab='')
title(main='House Price Index from 1975')
acf(hpi,ylab='',main='ACF')
x = parzen.wge(hpi)
```

The realization has a similar upward trend to the Housing Sale Price. It seems very smooth though, very little variance. Because of the upward trend though, it shows evidence against stationarity.

The ACF and Parzen Window are also similar to those of Housing Sale Price.

Not all the data matches up by date range or by frequency of observation (monthly vs quarterly).  To make everything consistent, we will only use quarterly data going from Q1 1975 to Q2 2024.

# AR(I)MA Model Investigation

## First ARIMA Model

The first model we looked at was an ARIMA model.  From the EDA, we saw that because of the increasing variance, we should look at the log of the Housing Sales Price.  And since the Parzen Window had a prominent peak at 0 and very slowly dampening autocorrelations, we should take the first difference.

```{r Difference Median Housing Price}
log.mhp = fed_housing_data$Median_Sales_Price
mhp = exp(log.mhp)
d.log.mhp = artrans.wge(log.mhp,1)
```

The difference looks more like white noise without an obvious pattern to the ACF.  Next we will try modeling the differenced data as an ARMA model.

```{r ARIMA(p,1,q) p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 2.

We will evaluate our models with a short term horizon of 1 year (4 quarters) and a long term horizon of 5 years (20 quarters).  We calcluated a Rolling Window RMSE for both of those horizons (using the logged data) and calculated an ASE for both of those horizons as well (taking the exponential to get back to the original data).

```{r ARIMA(p,1,q) metrics}
h.short = 4
h.long = 20
l = length(mhp)
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 84.37902
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 7091.032
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=1,phi=est$phi,theta=est$theta) # 0.036501
```
```{r ARIMA(p,1,q) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=1,phi=est$phi,theta=est$theta) # 0.13269
```
```{r ARIMA(p,1,q) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,1,2) model we get a short term Rolling Window RMSE of 0.037, a long term Rolling Window RMSE of 0.133, a short term ASE of 84.4 million and a long term ASE of 7.09 billion.

## ARMA Model

As a comparison, we wanted to look a simpler ARMA model.  

```{r ARMA p and q estimation}
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='aic')
aic5.wge(d.log.mhp,p=0:6,q=0:4,type='bic')
```

Both the AIC and BIC selection choose p = 2 and q = 2.

```{r ARMA metrics}
est = est.arma.wge(log.mhp,p=2,q=2)
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 112.4314
f = fore.arma.wge(log.mhp,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 9167.59
```
```{r ARMA Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,phi=est$phi,theta=est$theta) # 0.044488
```
```{r ARMA Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARMA Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,phi=est$phi,theta=est$theta) # 0.12779
```
```{r ARMA Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARMA(2,2) model we get a short term Rolling Window RMSE of 0.044, a long term Rolling Window RMSE of 0.128, a short term ASE of 112.4 million and a long term ASE of 9.17 billion.  Even though this model has a better long term Rolling Window RMSE than the ARIMA model, it did worse in the other metrics.

## ARIMA(p,2,q) model

Looking at the overfit factor table, we can see two prominent 0 frequency roots.  

```{r factor table}
est = est.ar.wge(log.mhp,p=12)
factor.wge(phi=est$phi)
```

Even though the differenced data didn't have slowly dampening autocorrelations and looked liked white noise, the original data had a pretty clear upward trend.  We thought that this warranted investigation into an ARIMA(p,2,q) model.

```{r ARIMA(p,2,q) p and q estimation}
d2.log.mhp = artrans.wge(d.log.mhp,1)
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='aic')
aic5.wge(d.log.mhp,p=0:4,q=0:2,type='bic')
```

Both the AIC and BIC selection choose p = 1 and q = 1.

```{r ARIMA(1,2,1) metrics}
est = est.arma.wge(d2.log.mhp,p=1,q=1)
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((mhp[(l-h.short+1):l]-exp(f$f))^2)/1e6
ase # 162.1383
f = fore.arima.wge(log.mhp,d=2,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((mhp[(l-h.long+1):l]-exp(f$f))^2)/1e6
ase # 6263.556
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.short,d=2,phi=est$phi,theta=est$theta) # 0.049161
```
```{r ARIMA(1,2,1) Rolling Window RMSE 1 Output}
r$rwRMSE
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2, fig.show='hide', results='hide'}
r = roll.win.rmse.wge(log.mhp,h.long,d=2,phi=est$phi,theta=est$theta) # 0.1809277
```
```{r ARIMA(1,2,1) Rolling Window RMSE 2 Output}
r$rwRMSE
```

For the ARIMA(1,2,1) model we get a short term Rolling Window RMSE of 0.049, a long term Rolling Window RMSE of 0.181, a short term ASE of 162.4 million and a long term ASE of 6.26 billion.  Even though this model had the best long term ASE so far, it did worse in the other metrics.

Comparing all three ARMA/ARIMA models, we will choose the ARIMA(1,1,2).  

```{r ARIMA(1,1,2) model characteristics}
est = est.arma.wge(d.log.mhp,p=1,q=2)
f = fore.arima.wge(log.mhp,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
aic = est$aic
aic # -7.188559
resid = f$resid
xbar=est$xbar
xbar # 0.01211603
vara = est$avar
vara # 0.0007251235
est$phi # 0.6901326
est$theta # 0.9720137 -0.4007360
```

The model (for the log data) can be written as:
(1-B)(1-0.69B)(x_t-0.012) = (1-0.972B+0.401B^2)a_t 
$\sigma$_t^2 = 0.001

Now we will look at residuals to make sure that they are white noise.

```{r ARIMA(1,1,2) residual plots}
plotts.wge(resid)
acf(resid,lag.max=100) 
parzen.wge(resid)
```

They appear to be white noise, and the ACF has very few values above 0.2.  We will run a Ljung-Box test to be sure.

```{r ARIMA(1,2,2) Ljung test}
ljung.wge(resid,K=24,p=1,q=2) # p = 0.2272667, white
ljung.wge(resid,K=48,p=1,q=2) # p = 0.5584003, white
```

The test gives more evidence that the residuals are white noise.  We can also check if they are normally distributed with a histogram.

```{r ARIMA(1,1,2) residual histogram}
hist(resid)
```

We also want to make sure that the ACFs and Parzen Windows generated from ARIMA(1,2,2) models look similar to that of our original data.

```{r ARIMA(1,1,2) generated ACFs and Parzen Windows}
# Multiple ACFs 
set.seed(2)
sims = 10
ACF = acf(log.mhp, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)
for( i in 1: sims)
{
  ACF2 = acf(gen.arima.wge(l, phi = est$phi, theta=est$theta,d=1, plot="FALSE"), plot = "FALSE")
  lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

# Multiple Parzen 
set.seed(3)
sims = 10
SpecDen = parzen.wge(log.mhp, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6)
for( i in 1: sims)
{
  SpecDen2 = parzen.wge(gen.aruma.wge(l,phi=est$phi, theta=est$theta,d=1, plot ="FALSE"), plot = "FALSE")
  lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}
```

The ACFs don't match up exactly, but have the same slowly dampening behavior.  The Parzen Windows are very close to the original.

# Exogenous Variable Forecasts

In order to use the Exogenous variables in either the MLR or Multi-Variate MLP, it is more realistic to make predictions for them first.  

For error metrics, we will focus on the ASE for short and long term predictions.  This is because we are specifically using these short and long term predictions in future models.

## Home Ownership Rate

The Home Ownership Rate looked like either an ARMA or an ARIMA(p,1,q) might be appropriate.

```{r re-plot Home Ownership Rate}
x = fed_housing_data$Ownership_Rate 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```

We will start with the ARIMA model.

```{r AIC for Home Ownership Rate ARIMA}
aic5.wge(d,p=0:10,q=0:4,type='aic') # 8/1 best
```

The model with the best AIC is p = 8, q = 1.

```{r Home Ownership Rate ARIMA(8,1,1) metrics}
est = est.arma.wge(d,p=8,q=1)
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.03083697
f = fore.arima.wge(x,d=1,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 2.077416
```

This model has a short term ASE of 0.031 and a long term ASE of 2.08.

Next will compare this to an ARMA model.

```{r AIC for Home Ownership Rate ARMA}
aic5.wge(x,p=0:16,q=0:4,type='aic') # 9/1 best
```

The model it chose had p = 9 and q = 1.

```{r Home Ownership Rate ARMA(9,1) metrics}
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)
ase # 0.02468854
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)
ase # 1.108741
```

This model had better ASE values with 0.025 for short term and 1.11 for long term.  This will be the predictions we use for our MLR and MLP models.

```{r Home Ownership Rate creating forecast predictions}
fed_housing_data_short = fed_housing_data
fed_housing_data_long = fed_housing_data
x = fed_housing_data$Ownership_Rate 
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
hor.pred.short = f$f
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
hor.pred.long = f$f
fed_housing_data_short$Ownership_Rate[(l-h.short+1):l] = hor.pred.short
fed_housing_data_long$Ownership_Rate[(l-h.long+1):l] = hor.pred.long
```

## Housing Units Completed

The Housing Units Completed variable looked like either an ARMA or an ARIMA(p,1,q) might be appropriate for forecasting.

```{r re-plot Housing Units Completed}
x = fed_housing_data$Housing_Units_Completed 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```

However, this looks like the autocorrelations dampen even less slowly than the Home Ownership Rate, meaning that an ARIMA(p,1,q) model would be even less appropriate.  Since we chose the ARMA model for Home Ownership Rate over the ARIMA model.  We will just focus on the ARMA model for Housing Units Complete.

```{r AIC for Housing Units Completed}
aic5.wge(x,p=0:10,q=0:1,type='aic') # 10/0 best
```

The highest AIC value was for p = 10 and q = 0.

```{r Housing Units Completed AR(10) metrics}
est = est.ar.wge(x,p=10)
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)/1e3
ase # 12.98288
f = fore.arima.wge(x,phi=est$phi,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)/1e3
ase # 8.546039
```

This gives a short term ASE of 13.0k and a long term ASE of 8.5k.  Playing around with some of the other output from the aic5 output showed that an ARMA(9,1) model does slightly better for long term predictions and slightly worse for short term predictions.

```{r Housing Units Completed ARMA(9,1) metrics}
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
ase = mean((x[(l-h.short+1):l]-f$f)^2)/1e3
ase # 13.44075
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
ase = mean((x[(l-h.long+1):l]-f$f)^2)/1e3
ase # 8.516217
```

We will proceed with the ARMA(9,1) forecast for our future models that use exogenous variables.

```{r Housing Units Completed creating forecast predictions}
x = fed_housing_data$Housing_Units_Completed  
est = est.arma.wge(x,p=9,q=1)
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.short,lastn=TRUE)
huc.pred.short = f$f
f = fore.arima.wge(x,phi=est$phi,theta=est$theta,n.ahead=h.long,lastn=TRUE)
huc.pred.long = f$f
fed_housing_data_short$Housing_Units_Completed[(l-h.short+1):l] = huc.pred.short
fed_housing_data_long$Housing_Units_Completed[(l-h.long+1):l] = huc.pred.long
```

## Supply of New Houses

Similar to Housing Units Completed, an ARMA model seems like it would be most appropriate for Supply of New Houses.

```{r re-plot Supply of New Houses}
x = fed_housing_data$Supply_New_Houses 
plotts.sample.wge(x)
d = artrans.wge(x,1)
```


